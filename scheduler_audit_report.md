# 📅 定时任务审计报告 (Scheduler Audit Report)

> **生成时间**: 2026-01-24
> **审计对象**: TradingAgents-CN 后端调度系统
> **数据来源**: 源代码 (`src/app/main.py`)

## 1. 核心业务任务

### ⚡ 1.1 实时行情入库 (`quotes_ingestion_service`)
- **触发类型**: `Interval` (间隔触发)
- **执行频率**: 每 **60秒** (默认配置，可调整)
- **业务逻辑**: 仅在交易时间段内运行。捕捉 A股/港股 的最新市场快照并存入 MongoDB `market_quotes` 集合，为实时分析提供数据流。

### 📊 1.2 股票基础信息同步 (`basics_sync_service`)
- **触发类型**: `Cron` (定时触发)
- **执行时间**: 每天 **06:30**
- **业务逻辑**: 执行全量股票列表更新。
  - **自动降级机制**: 优先尝试 Tushare -> 失败则降级至 AKShare ->最后兜底 BaoStock。
  - **目的**: 确保当日交易前股票代码表是最新的。

---

## 2. 新闻数据同步体系 (三级梯队)

为了在数据时效性与 API 配额/反爬虫限制之间取得平衡，系统实现了三级同步策略：

### 🥇 Tier 1: 关键点多源同步 (`multi_source_news_sync`)
- **执行时间**: 每天 **05:00, 10:00, 21:00**
- **覆盖时段**: 盘前晨报、盘中动态、美股盘前。
- **业务逻辑**: 聚合 AKShare、RSS 源及外部 API，以最小的请求次数覆盖最重要的市场时段。

### 🥈 Tier 2: AKShare 兜底同步 (`news_sync`)
- **执行时间**: 每 **4小时** 整点 (`0 */4 * * *`)
- **业务逻辑**: 传统的定点爬取任务，作为稳健的数据保底机制，确保全天候无数据断层。

### 🥉 Tier 3: 爬虫错峰同步 (`scraper_news_sync`)
- **执行时间**: 每 **4小时** 半点 (`30 */4 * * *`)
- **业务逻辑**: 
  - 启动 Playwright 爬虫进行深度抓取。
  - **错峰设计**: 特意安排在整点后 30 分钟执行，避免与 Tier 2 任务争抢系统资源或触发源站并发限制。

---

## 3. 系统维护与健康守护

### 🧹 3.1 过期数据清理 (`scraper_news_cleanup`)
- **执行时间**: 每天 **03:00**
- **业务逻辑**: 自动清理 **90天前** 的爬虫新闻数据，防止数据库无限膨胀，保持检索性能。

### 🧟 3.2 僵尸任务检测 (`check_zombie_tasks`)
- **触发类型**: `Interval`
- **执行频率**: 每 **5分钟**
- **业务逻辑**: 扫描系统中的"僵尸任务"（即状态为 Running 但持续时间超过 30 分钟的任务）。这通常意味着进程意外终止或死锁。系统会自动将其标记为 Failed，防止调度器阻塞。

---

---

## 4. 特殊机制：港美股数据 (yfinance)

yfinance 比较特殊，它没有定时任务，而是采用 **“即查即用”** 的模式。

*   **⚡ 触发方式**: 当你点击“分析”按钮时，系统立即去抓取最新数据。
*   **🍔 大白话比喻**:
    *   **A股 (AKShare)** 就像 **“订牛奶”**：每天早上固定时间送到门口（存入数据库），想喝随时拿。
    *   **港美股 (yfinance)** 就像 **“点外卖”**：你想吃的时候（发起分析）才下单，骑手（爬虫）现跑去买。
*   **❄️ 聪明设计**: 买回来的数据会放进 **“冰箱” (缓存)** 保存一天。如果你今天再查同一只股票，系统直接从冰箱拿，不用再等骑手跑一趟，既快又省流量。

---

## 5. 审计结论
1.  **逻辑闭环**: 所有任务均有明确的启动时间与兜底策略。
2.  **资源友好**: 爬虫与 API 调用采用了错峰排期（整点 vs 半点）。
3.  **自我修复**: 具备僵尸进程自动清理机制，保障系统长期稳定运行。
4.  **全覆盖**: A股定时同步，港美股按需即取，两者互补，覆盖全球市场。
