#!/usr/bin/env python3
import json
import requests
"""
ç»Ÿä¸€æ–°é—»åˆ†æå·¥å…·
æ•´åˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ç­‰ä¸åŒå¸‚åœºçš„æ–°é—»è·å–é€»è¾‘åˆ°ä¸€ä¸ªå·¥å…·å‡½æ•°ä¸­
è®©å¤§æ¨¡å‹åªéœ€è¦è°ƒç”¨ä¸€ä¸ªå·¥å…·å°±èƒ½è·å–æ‰€æœ‰ç±»å‹è‚¡ç¥¨çš„æ–°é—»æ•°æ®
"""

import logging
from datetime import datetime
import re

logger = logging.getLogger(__name__)

class UnifiedNewsAnalyzer:
    """ç»Ÿä¸€æ–°é—»åˆ†æå™¨ï¼Œæ•´åˆæ‰€æœ‰æ–°é—»è·å–é€»è¾‘"""
    
    def __init__(self, toolkit):
        """åˆå§‹åŒ–ç»Ÿä¸€æ–°é—»åˆ†æå™¨
        
        Args:
            toolkit: åŒ…å«å„ç§æ–°é—»è·å–å·¥å…·çš„å·¥å…·åŒ…
        """
        self.toolkit = toolkit
        
    def get_stock_news_unified(self, stock_code: str, max_news: int = 10, model_info: str = "") -> dict:
        """
        ç»Ÿä¸€æ–°é—»è·å–æ¥å£
        æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹å¹¶è·å–ç›¸åº”æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡
            model_info: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
            
        Returns:
            dict: åŒ…å«æ–°é—»å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å¼€å§‹è·å– {stock_code} çš„æ–°é—»ï¼Œæ¨¡å‹: {model_info}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– å½“å‰æ¨¡å‹ä¿¡æ¯: {model_info}")
        
        # è¯†åˆ«è‚¡ç¥¨ç±»å‹
        stock_type = self._identify_stock_type(stock_code)
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ç±»å‹: {stock_type}")
        
        # æ ¹æ®è‚¡ç¥¨ç±»å‹è°ƒç”¨ç›¸åº”çš„è·å–æ–¹æ³•
        if stock_type == "Aè‚¡":
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        elif stock_type == "æ¸¯è‚¡":
            result_str = self._get_hk_share_news(stock_code, max_news, model_info)
        elif stock_type == "ç¾è‚¡":
            result_str = self._get_us_share_news(stock_code, max_news, model_info)
        else:
            # é»˜è®¤ä½¿ç”¨Aè‚¡é€»è¾‘
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        
        # ğŸ” æ„å»ºâ€œæ»¡è¡€ç‰ˆâ€æ•°æ®é›†ï¼šæ³¨å…¥å—å‘/åŒ—å‘èµ„é‡‘æµæ•°æ® (ä»…é™Aè‚¡/æ¸¯è‚¡)
        fund_flow_str = ""
        if stock_type in ["Aè‚¡", "æ¸¯è‚¡"]:
            try:
                from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
                provider = get_akshare_provider()
                # ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„èµ„é‡‘æµè·å–
                fund_flow_str = provider.get_hsgt_fund_flow_sync(stock_code)
                
                if fund_flow_str:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æˆåŠŸæ³¨å…¥èµ„é‡‘æµæ•°æ®")
                    result_str = fund_flow_str + "\n\n" + result_str
            except Exception as e:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ èµ„é‡‘æµæ³¨å…¥å¤±è´¥: {e}")

        # ğŸ” æ·»åŠ è¯¦ç»†çš„ç»“æœè°ƒè¯•æ—¥å¿—
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æ»¡è¡€ç‰ˆæ•°æ®é›†æ„å»ºå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_str)} å­—ç¬¦")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ è¿”å›ç»“æœé¢„è§ˆ (å‰1000å­—ç¬¦): {result_str[:1000]}")
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè®°å½•è­¦å‘Š
        if not result_str or len(result_str.strip()) < 50:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ è¿”å›ç»“æœå¼‚å¸¸çŸ­æˆ–ä¸ºç©ºï¼")
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“ å®Œæ•´ç»“æœå†…å®¹: '{result_str}'")
        
        # æ„é€ è¿”å›å­—å…¸
        return {
            "status": "success" if result_str and len(result_str) > 50 else "warning",
            "content": result_str,
            "stock_type": stock_type,
            "ticker": stock_code,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def _identify_stock_type(self, stock_code: str) -> str:
        """è¯†åˆ«è‚¡ç¥¨ç±»å‹"""
        stock_code = stock_code.upper().strip()
        
        # Aè‚¡åˆ¤æ–­
        if re.match(r'^(00|30|60|68)\d{4}$', stock_code):
            return "Aè‚¡"
        elif re.match(r'^(SZ|SH)\d{6}$', stock_code):
            return "Aè‚¡"
        
        # æ¸¯è‚¡åˆ¤æ–­
        elif re.match(r'^\d{4,5}\.HK$', stock_code):
            return "æ¸¯è‚¡"
        elif re.match(r'^\d{4,5}$', stock_code) and len(stock_code) <= 5:
            return "æ¸¯è‚¡"
        
        # ç¾è‚¡åˆ¤æ–­
        elif re.match(r'^[A-Z]{1,5}$', stock_code):
            return "ç¾è‚¡"
        elif '.' in stock_code and not stock_code.endswith('.HK'):
            return "ç¾è‚¡"
        
        # é»˜è®¤æŒ‰Aè‚¡å¤„ç†
        else:
            return "Aè‚¡"
    
    def _search_news_with_serper(self, query: str, period: str = "qdr:w") -> str:
        """
        ä½¿ç”¨Serper APIæœç´¢æ–°é—»
        Args:
            query: æœç´¢å…³é”®è¯
            period: æ—¶é—´èŒƒå›´ï¼Œé»˜è®¤ä¸ºè¿‡å»ä¸€å‘¨ (qdr:w)ï¼Œå¯é€‰ qdr:d (ä¸€å¤©), qdr:m (ä¸€æœˆ)
        """
        try:
            import os
            api_key = os.getenv("SERPER_API_KEY")
            if not api_key:
                return ""
                
            url = "https://google.serper.dev/search"
            headers = {
                'X-API-KEY': api_key,
                'Content-Type': 'application/json'
            }
            
            payload = json.dumps({
                "q": query,
                "tbs": period,
                "num": 10
            })
            
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code != 200:
                return ""
                
            results = response.json().get('organic', [])
            if not results:
                return ""
                
            formatted = []
            for item in results:
                title = item.get('title', '')
                snippet = item.get('snippet', '')
                link = item.get('link', '')
                date = item.get('date', '')
                formatted.append(f"### {title}\n- **æ¥æº**: {link}\n- **æ—¶é—´**: {date}\n- **æ‘˜è¦**: {snippet}\n")
                
            return "\n".join(formatted)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Serperæœç´¢å¤±è´¥: {e}")
            return ""

    def _get_company_name_from_code(self, stock_code: str) -> str:
        """
        æ ¹æ®è‚¡ç¥¨ä»£ç è·å–å…¬å¸åç§°
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            str: å…¬å¸åç§°ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # ç®€å•çš„æ˜ å°„è¡¨ï¼ˆå¸¸è§è‚¡ç¥¨ï¼‰
        stock_name_map = {
            '09618': 'äº¬ä¸œé›†å›¢',
            '9618': 'äº¬ä¸œé›†å›¢',
            'JD': 'äº¬ä¸œ',
            '00700': 'è…¾è®¯æ§è‚¡',
            '0700': 'è…¾è®¯æ§è‚¡',
            'BABA': 'é˜¿é‡Œå·´å·´',
            '09988': 'é˜¿é‡Œå·´å·´',
            'AAPL': 'è‹¹æœ',
            'TSLA': 'ç‰¹æ–¯æ‹‰',
            'NVDA': 'è‹±ä¼Ÿè¾¾',
            '000001': 'å¹³å®‰é“¶è¡Œ',
            '600519': 'è´µå·èŒ…å°',
        }
        
        # æ ‡å‡†åŒ–ä»£ç 
        clean_code = stock_code.replace('.HK', '').replace('.SH', '').replace('.SZ', '')
        
        # æŸ¥æ‰¾æ˜ å°„
        company_name = stock_name_map.get(clean_code, '')
        
        if company_name:
            logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ä»£ç  {stock_code} æ˜ å°„åˆ°å…¬å¸åç§°: {company_name}")
        else:
            logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ä»£ç  {stock_code} æœªæ‰¾åˆ°å…¬å¸åç§°æ˜ å°„")
        
        return company_name

    def _get_news_from_database(self, stock_code: str, max_news: int = 10, company_name: str = "") -> str:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒå†…å®¹ç›¸å…³æ€§æŸ¥è¯¢ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡
            company_name: å…¬å¸åç§°ï¼ˆç”¨äºå†…å®¹åŒ¹é…ï¼‰

        Returns:
            str: æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰æ–°é—»åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            from tradingagents.dataflows.cache.app_adapter import get_mongodb_client
            from datetime import timedelta

            # ğŸ”§ ç¡®ä¿ max_news æ˜¯æ•´æ•°ï¼ˆé˜²æ­¢ä¼ å…¥æµ®ç‚¹æ•°ï¼‰
            max_news = int(max_news)

            client = get_mongodb_client()
            if not client:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ— æ³•è¿æ¥åˆ°MongoDB")
                return ""

            db = client.get_database('tradingagents')
            collection = db.stock_news

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            # æŸ¥è¯¢æœ€è¿‘30å¤©çš„æ–°é—»ï¼ˆæ‰©å¤§æ—¶é—´èŒƒå›´ï¼‰
            thirty_days_ago = datetime.now() - timedelta(days=30)

            # ğŸ”¥ æ”¹è¿›ï¼šæ„å»ºå…³é”®è¯åˆ—è¡¨ï¼ˆæ”¯æŒå†…å®¹ç›¸å…³æ€§æŸ¥è¯¢ï¼‰
            keywords = [stock_code, clean_code]
            
            if company_name:
                # æ·»åŠ å…¬å¸åç§°ç›¸å…³å…³é”®è¯
                keywords.append(company_name)
                # å»é™¤"é›†å›¢"ã€"è‚¡ä»½"ç­‰åç¼€
                clean_name = company_name.replace('é›†å›¢', '').replace('è‚¡ä»½', '').replace('æœ‰é™å…¬å¸', '')
                if clean_name != company_name:
                    keywords.append(clean_name)
                
                # æ·»åŠ ç›¸å…³ä¸šåŠ¡å…³é”®è¯ï¼ˆé’ˆå¯¹å¤§å…¬å¸ï¼‰
                if clean_name:
                    keywords.extend([
                        f'{clean_name}ç‰©æµ',
                        f'{clean_name}é›¶å”®',
                        f'{clean_name}ç§‘æŠ€'
                    ])
            
            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            keyword_pattern = '|'.join([k for k in keywords if k])
            
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” æŸ¥è¯¢å…³é”®è¯: {keywords[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª

            # ğŸ”¥ æ”¹è¿›ï¼šåˆ†ä¸¤æ­¥æŸ¥è¯¢ï¼Œå…ˆè·å–ä¸“å±æ–°é—»ï¼Œå†è¡¥å……RSSé€šç”¨æ–°é—»
            news_items = []
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢ä¸“å±æ–°é—»ï¼ˆä¼˜å…ˆçº§1-5ï¼‰
            specific_queries = [
                # ä¼˜å…ˆçº§1: ç²¾ç¡®åŒ¹é…symbol + æ—¶é—´èŒƒå›´
                {'symbol': clean_code, 'publish_time': {'$gte': thirty_days_ago}},
                {'symbol': stock_code, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§2: æ ‡é¢˜åŒ…å«å…³é”®è¯ + æ—¶é—´èŒƒå›´
                {'title': {'$regex': keyword_pattern, '$options': 'i'}, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§3: å†…å®¹åŒ…å«å…³é”®è¯ + æ—¶é—´èŒƒå›´
                {'content': {'$regex': keyword_pattern, '$options': 'i'}, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§4: ç²¾ç¡®åŒ¹é…symbolï¼ˆä¸é™æ—¶é—´ï¼‰
                {'symbol': clean_code},
                {'symbol': stock_code},
                
                # ä¼˜å…ˆçº§5: æ ‡é¢˜åŒ…å«å…³é”®è¯ï¼ˆä¸é™æ—¶é—´ï¼‰
                {'title': {'$regex': keyword_pattern, '$options': 'i'}},
            ]
            
            # å°è¯•è·å–ä¸“å±æ–°é—»
            for query in specific_queries:
                cursor = collection.find(query).sort('publish_time', -1).limit(max_news)
                news_items = list(cursor)
                if news_items:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æ‰¾åˆ° {len(news_items)} æ¡ä¸“å±æ–°é—»")
                    logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æŸ¥è¯¢æ¡ä»¶: {query}")
                    break
            
            # ç¬¬äºŒæ­¥ï¼šå®æ—¶è·å–AKShareæ–°é—»ï¼ˆå¦‚æœä¸“å±æ–°é—»ä¸è¶³max_newsæ¡ï¼‰
            if len(news_items) < max_news:
                try:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ ä¸“å±æ–°é—»ä¸è¶³ï¼Œå°è¯•å®æ—¶è·å–AKShareæ–°é—»...")
                    
                    # åŠ¨æ€å¯¼å…¥AKShareé€‚é…å™¨
                    import asyncio
                    from app.worker.news_adapters.akshare_adapter import AKShareAdapter
                    
                    # åˆ›å»ºé€‚é…å™¨å¹¶è·å–å®æ—¶æ–°é—»
                    akshare = AKShareAdapter()
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå§‹åŒ–
                    loop = None
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # åˆå§‹åŒ–å¹¶è·å–æ–°é—»
                    if not loop.is_running():
                        loop.run_until_complete(akshare.initialize())
                        realtime_limit = min(10, max_news - len(news_items))  # æœ€å¤šè·å–10æ¡å®æ—¶æ–°é—»
                        realtime_news = loop.run_until_complete(akshare.get_news(stock_code, limit=realtime_limit))
                    else:
                        # å¦‚æœäº‹ä»¶å¾ªç¯å·²åœ¨è¿è¡Œï¼Œä½¿ç”¨åŒæ­¥æ–¹å¼
                        import nest_asyncio
                        nest_asyncio.apply()
                        loop.run_until_complete(akshare.initialize())
                        realtime_limit = min(10, max_news - len(news_items))
                        realtime_news = loop.run_until_complete(akshare.get_news(stock_code, limit=realtime_limit))
                    
                    if realtime_news:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”¥ å®æ—¶è·å– {len(realtime_news)} æ¡AKShareæ–°é—»")
                        # è½¬æ¢ä¸ºMongoDBæ ¼å¼
                        for news in realtime_news:
                            news_items.append(news)
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š åˆå¹¶åå…± {len(news_items)} æ¡æ–°é—»ï¼ˆä¸“å±+å®æ—¶AKShareï¼‰")
                    else:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ å®æ—¶AKShareæœªè¿”å›æ–°é—»")
                        
                except Exception as e:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ å®æ—¶è·å–AKShareæ–°é—»å¤±è´¥: {e}")
            
            # ç¬¬ä¸‰æ­¥ï¼šè¡¥å……RSSé€šç”¨æ–°é—»ï¼ˆå¦‚æœä»ç„¶ä¸è¶³max_newsæ¡ï¼‰
            if len(news_items) < max_news:
                rss_limit = max_news - len(news_items)  # è®¡ç®—è¿˜éœ€è¦å¤šå°‘æ¡
                rss_query = {'symbol': 'GENERAL', 'source': 'RSS', 'publish_time': {'$gte': thirty_days_ago}}
                rss_cursor = collection.find(rss_query).sort('publish_time', -1).limit(rss_limit)
                rss_items = list(rss_cursor)
                
                if rss_items:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° è¡¥å…… {len(rss_items)} æ¡RSSé€šç”¨æ–°é—»")
                    news_items.extend(rss_items)
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æœ€ç»ˆåˆå¹¶å…± {len(news_items)} æ¡æ–°é—»ï¼ˆä¸“å±+å®æ—¶+RSSï¼‰")

            if not news_items:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ° {stock_code} æˆ– {company_name} çš„ç›¸å…³æ–°é—»")
                return ""

            # æ ¼å¼åŒ–æ–°é—»
            report = f"# {stock_code} æœ€æ–°æ–°é—» (æ•°æ®åº“ç¼“å­˜)\n\n"
            report += f"ğŸ“… æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            report += f"ğŸ“Š æ–°é—»æ•°é‡: {len(news_items)} æ¡\n\n"

            for i, news in enumerate(news_items, 1):
                title = news.get('title', 'æ— æ ‡é¢˜')
                content = news.get('content', '') or news.get('summary', '')
                source = news.get('source', 'æœªçŸ¥æ¥æº')
                publish_time = news.get('publish_time', datetime.now())
                sentiment = news.get('sentiment', 'neutral')

                # æƒ…ç»ªå›¾æ ‡
                sentiment_icon = {
                    'positive': 'ğŸ“ˆ',
                    'negative': 'ğŸ“‰',
                    'neutral': 'â–'
                }.get(sentiment, 'â–')

                report += f"## {i}. {sentiment_icon} {title}\n\n"
                report += f"**æ¥æº**: {source} | **æ—¶é—´**: {publish_time.strftime('%Y-%m-%d %H:%M') if isinstance(publish_time, datetime) else publish_time}\n"
                report += f"**æƒ…ç»ª**: {sentiment}\n\n"

                if content:
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    content_preview = content[:500] + '...' if len(content) > 500 else content
                    report += f"{content_preview}\n\n"

                report += "---\n\n"

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æˆåŠŸä»æ•°æ®åº“è·å–å¹¶æ ¼å¼åŒ– {len(news_items)} æ¡æ–°é—»")
            return report

        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ä»æ•°æ®åº“è·å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return ""

    def _sync_news_from_akshare(self, stock_code: str, max_news: int = 10) -> bool:
        """
        ä»AKShareåŒæ­¥æ–°é—»åˆ°æ•°æ®åº“ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
        ä½¿ç”¨åŒæ­¥çš„æ•°æ®åº“å®¢æˆ·ç«¯å’Œæ–°çº¿ç¨‹ä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            bool: æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        try:
            import asyncio
            import concurrent.futures

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”„ å¼€å§‹åŒæ­¥ {clean_code} çš„æ–°é—»...")

            # ğŸ”¥ åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä½¿ç”¨åŒæ­¥æ•°æ®åº“å®¢æˆ·ç«¯
            def run_sync_in_new_thread():
                """åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯å¹¶è¿è¡ŒåŒæ­¥ä»»åŠ¡"""
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

                try:
                    # å®šä¹‰å¼‚æ­¥è·å–æ–°é—»ä»»åŠ¡
                    async def get_news_task():
                        try:
                            # åŠ¨æ€å¯¼å…¥ AKShare providerï¼ˆæ­£ç¡®çš„å¯¼å…¥è·¯å¾„ï¼‰
                            from tradingagents.dataflows.providers.china.akshare import AKShareProvider

                            # åˆ›å»º provider å®ä¾‹
                            provider = AKShareProvider()

                            # è°ƒç”¨ provider è·å–æ–°é—»
                            news_data = await provider.get_stock_news(
                                symbol=clean_code,
                                limit=max_news
                            )

                            return news_data

                        except Exception as e:
                            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ è·å–æ–°é—»å¤±è´¥: {e}")
                            import traceback
                            logger.error(traceback.format_exc())
                            return None

                    # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è·å–æ–°é—»
                    news_data = new_loop.run_until_complete(get_news_task())

                    if not news_data:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æœªè·å–åˆ°æ–°é—»æ•°æ®")
                        return False

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¥ è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                    # ğŸ”¥ ä½¿ç”¨åŒæ­¥æ–¹æ³•ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¸ä¾èµ–äº‹ä»¶å¾ªç¯ï¼‰
                    from app.services.news_data_service import NewsDataService

                    news_service = NewsDataService()
                    saved_count = news_service.save_news_data_sync(
                        news_data=news_data,
                        data_source="akshare",
                        market="CN"
                    )

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸ: {saved_count} æ¡æ–°é—»")
                    return saved_count > 0

                finally:
                    # æ¸…ç†äº‹ä»¶å¾ªç¯
                    new_loop.close()

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª")
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_sync_in_new_thread)
                result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                return result

        except concurrent.futures.TimeoutError:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
            return False
        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _get_a_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–Aè‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–Aè‚¡ {stock_code} æ–°é—»")

        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")

        # ä¼˜å…ˆçº§0: ä»æ•°æ®åº“è·å–æ–°é—»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” ä¼˜å…ˆä»æ•°æ®åº“è·å– {stock_code} çš„æ–°é—»...")
            # è·å–å…¬å¸åç§°ç”¨äºå†…å®¹åŒ¹é…
            company_name = self._get_company_name_from_code(stock_code)
            db_news = self._get_news_from_database(stock_code, max_news, company_name)
            if db_news:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜", model_info)
            else:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰ {stock_code} çš„æ–°é—»ï¼Œå°è¯•åŒæ­¥...")

                # ğŸ”¥ æ•°æ®åº“æ²¡æœ‰æ•°æ®æ—¶ï¼Œè°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥æ–°é—»
                try:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ è°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥ {stock_code} çš„æ–°é—»...")
                    synced_news = self._sync_news_from_akshare(stock_code, max_news)

                    if synced_news:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸï¼Œé‡æ–°ä»æ•°æ®åº“è·å–...")
                        # é‡æ–°ä»æ•°æ®åº“è·å–
                        company_name = self._get_company_name_from_code(stock_code)
                        db_news = self._get_news_from_database(stock_code, max_news, company_name)
                        if db_news:
                            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥åæ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                            return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜(æ–°åŒæ­¥)", model_info)
                    else:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡æœªè¿”å›æ–°é—»æ•°æ®")

                except Exception as sync_error:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡è°ƒç”¨å¤±è´¥: {sync_error}")

                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥åä»æ— æ•°æ®ï¼Œå°è¯•å…¶ä»–æ•°æ®æº...")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“æ–°é—»è·å–å¤±è´¥: {e}")

        # ä¼˜å…ˆçº§1: ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»
        try:
            if hasattr(self.toolkit, 'get_realtime_stock_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_realtime_stock_news.invoke({"ticker": stock_code, "curr_date": curr_date})
                
                # ğŸ” è¯¦ç»†è®°å½•ä¸œæ–¹è´¢å¯Œè¿”å›çš„å†…å®¹
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š ä¸œæ–¹è´¢å¯Œè¿”å›å†…å®¹é•¿åº¦: {len(result) if result else 0} å­—ç¬¦")
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ ä¸œæ–¹è´¢å¯Œè¿”å›å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦): {result[:500] if result else 'None'}")
                
                if result and len(result.strip()) > 100:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»", model_info)
                else:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ ä¸œæ–¹è´¢å¯Œæ–°é—»å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: Googleæ–°é—»ï¼ˆä¼˜å…ˆä½¿ç”¨Serper APIï¼Œå›é€€åˆ°æ™®é€šçˆ¬è™«ï¼‰
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleæ–°é—» (Serper API)...")
            query = f"{stock_code} è‚¡ç¥¨ æ–°é—» è´¢æŠ¥"
            
            # 1. å°è¯•ä½¿ç”¨ Serper API
            serper_result = self._search_news_with_serper(query, period="qdr:w")
            if serper_result and len(serper_result) > 50:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Serperæ–°é—»è·å–æˆåŠŸ: {len(serper_result)} å­—ç¬¦")
                return self._format_news_result(serper_result, "Google/Serperæ–°é—»", model_info)
            
            # 2. å›é€€åˆ°æ™®é€šçˆ¬è™«
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Serperæ— ç»“æœï¼Œå°è¯•Googleæ–°é—»çˆ¬è™«...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ–°é—»çˆ¬è™«è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleæ–°é—»(çˆ¬è™«)", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleæ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIå…¨çƒæ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIæ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIå…¨çƒæ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIæ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–Aè‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def _get_hk_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–æ¸¯è‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–æ¸¯è‚¡ {stock_code} æ–°é—»")
        
        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        
        # ä¼˜å…ˆçº§0: ä»æ•°æ®åº“è·å–æ–°é—»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” ä¼˜å…ˆä»æ•°æ®åº“è·å– {stock_code} çš„æ–°é—»...")
            company_name = self._get_company_name_from_code(stock_code)
            db_news = self._get_news_from_database(stock_code, max_news, company_name)
            if db_news:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜", model_info)
            else:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰ {stock_code} çš„æ–°é—»ï¼Œå°è¯•åŒæ­¥...")
                
                # ğŸ”¥ æ•°æ®åº“æ²¡æœ‰æ•°æ®æ—¶ï¼Œè°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥æ–°é—»
                try:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ è°ƒç”¨åŒæ­¥æœåŠ¡åŒæ­¥ {stock_code} çš„æ–°é—»...")
                    synced_news = self._sync_news_from_akshare(stock_code, max_news)
                    
                    if synced_news:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸï¼Œé‡æ–°ä»æ•°æ®åº“è·å–...")
                        # é‡æ–°ä»æ•°æ®åº“è·å–
                        company_name = self._get_company_name_from_code(stock_code)
                        db_news = self._get_news_from_database(stock_code, max_news, company_name)
                        if db_news:
                            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥åæ•°æ®åº“æ–°é—»è·å–æˆåŠŸ: {len(db_news)} å­—ç¬¦")
                            return self._format_news_result(db_news, "æ•°æ®åº“ç¼“å­˜(æ–°åŒæ­¥)", model_info)
                    else:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡æœªè¿”å›æ–°é—»æ•°æ®")
                
                except Exception as sync_error:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥æœåŠ¡è°ƒç”¨å¤±è´¥: {sync_error}")
                
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ åŒæ­¥åä»æ— æ•°æ®ï¼Œå°è¯•å…¶ä»–æ•°æ®æº...")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§1: Alpha Vantageä¸ªè‚¡æ–°é—»ï¼ˆæ–°å¢ï¼‰âœ¨
        try:
            from tradingagents.tools.alpha_vantage_news import get_alpha_vantage_news, format_alpha_vantage_news
            
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Alpha Vantageä¸ªè‚¡æ–°é—»...")
            av_news = get_alpha_vantage_news(ticker=stock_code, limit=15)
            
            if av_news and len(av_news) > 0:
                # æ ¼å¼åŒ–ä¸ºmarkdown
                formatted = format_alpha_vantage_news(av_news, stock_code)
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Alpha Vantageä¸ªè‚¡æ–°é—»è·å–æˆåŠŸ: {len(av_news)}æ¡")
                return self._format_news_result(formatted, "Alpha Vantageä¸ªè‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Alpha Vantageä¸ªè‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: Googleæ–°é—»ï¼ˆä¼˜å…ˆä½¿ç”¨Serper APIï¼Œå›é€€åˆ°æ™®é€šçˆ¬è™«ï¼‰
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleæ¸¯è‚¡æ–°é—» (Serper API)...")
            query = f"{stock_code} æ¸¯è‚¡ é¦™æ¸¯è‚¡ç¥¨ æ–°é—»"
            
            # 1. å°è¯•ä½¿ç”¨ Serper API
            serper_result = self._search_news_with_serper(query, period="qdr:w")
            if serper_result and len(serper_result) > 50:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Serperæ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(serper_result)} å­—ç¬¦")
                return self._format_news_result(serper_result, "Google/Serperæ¸¯è‚¡æ–°é—»", model_info)
            
            # 2. å›é€€åˆ°æ™®é€šçˆ¬è™«
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Serperæ— ç»“æœï¼Œå°è¯•Googleæ–°é—»çˆ¬è™«...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ¸¯è‚¡æ–°é—»çˆ¬è™«è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleæ¸¯è‚¡æ–°é—»(çˆ¬è™«)", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleæ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIæ¸¯è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIæ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIæ¸¯è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIæ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: å®æ—¶æ–°é—»ï¼ˆå¦‚æœæ”¯æŒæ¸¯è‚¡ï¼‰
        try:
            if hasattr(self.toolkit, 'get_realtime_stock_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•å®æ—¶æ¸¯è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_realtime_stock_news.invoke({"ticker": stock_code, "curr_date": curr_date})
                if result and len(result.strip()) > 100:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… å®æ—¶æ¸¯è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "å®æ—¶æ¸¯è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å®æ—¶æ¸¯è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–æ¸¯è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def get_stock_sentiment_unified(
        self,
        ticker: str,
        curr_date: str
    ) -> dict:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        å¯¹äºAè‚¡å’Œæ¸¯è‚¡ï¼Œä½¿ç”¨Serper APIæŠ“å–é›ªçƒå’Œè‚¡å§çš„çœŸå®æ•£æˆ·è¯„è®º

        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001ã€0700.HKã€AAPLï¼‰
            curr_date: å½“å‰æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰

        Returns:
            dict: åŒ…å«æƒ…ç»ªåˆ†ææŠ¥å‘Šå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] åˆ†æè‚¡ç¥¨: {ticker}")

        try:
            from tradingagents.utils.stock_utils import StockUtils

            # è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹
            market_info = StockUtils.get_market_info(ticker)
            is_china = market_info['is_china']
            is_hk = market_info['is_hk']
            is_us = market_info['is_us']

            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] è‚¡ç¥¨ç±»å‹: {market_info['market_name']}")

            result_data = []
            
            # åˆå§‹åŒ–é»˜è®¤è¿”å›å­—å…¸
            response_dict = {
                "ticker": ticker,
                "stock_type": market_info['market_name'],
                "date": curr_date,
                "sentiment": "Neutral",
                "score": 0.5,
                "summary": "åˆ†æä¸­...",
                "confidence": "low",
                "content": ""
            }

            if is_china or is_hk:
                # ä¸­å›½Aè‚¡å’Œæ¸¯è‚¡ï¼šä½¿ç”¨Serper APIæœç´¢é›ªçƒå’Œè‚¡å§
                logger.info(f"ğŸ‡¨ğŸ‡³ğŸ‡­ğŸ‡° [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] ä½¿ç”¨Serperæœç´¢ä¸­æ–‡å¸‚åœºæƒ…ç»ª...")
                
                try:
                    import requests
                    import os
                    import re
                    
                    serper_api_key = os.getenv("SERPER_API_KEY")
                    if not serper_api_key:
                        raise ValueError("æœªé…ç½®SERPER_API_KEY")
                        
                    # å¤„ç†è‚¡ç¥¨åç§°
                    clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '').replace('.HK', '')
                    stock_name = market_info.get('name', '').replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                    
                    # ã€å®‰å…¨é”ã€‘ä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œé»˜è®¤ä¸º Tickerï¼Œé˜²æ­¢æŸ¥åº“å¤±è´¥å¯¼è‡´å˜é‡ç¼ºå¤±
                    real_name = clean_ticker
                    
                    # å°è¯•ä»æ•°æ®åº“è·å–çœŸå®å…¬å¸å
                    try:
                        # ã€å…³é”®ã€‘å±€éƒ¨å¼•ç”¨ï¼Œé˜²æ­¢ Circular Import å¯¼è‡´å…¨ç«™å´©æºƒ
                        from tradingagents.dataflows.interface import get_stock_name_by_ticker
                        
                        # å°è¯•æŸ¥è¯¢
                        name_from_db = get_stock_name_by_ticker(clean_ticker)
                        if name_from_db:
                            real_name = name_from_db
                            logger.info(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âœ… ä»æ•°æ®åº“è·å–åˆ°å…¬å¸å: {real_name}")
                        else:
                            logger.info(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âš ï¸ æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ° {clean_ticker} çš„å…¬å¸åï¼Œä½¿ç”¨ä»£ç : {real_name}")
                    except Exception as e:
                        # ã€å…³é”®ã€‘é™é»˜å¤±è´¥ï¼šå¦‚æœ‰ä»»ä½•æŠ¥é”™ï¼Œç›´æ¥å¿½ç•¥ï¼Œä»…æ‰“å°è­¦å‘Šï¼Œä¿è¯ç¨‹åºç»§ç»­è¿è¡Œ
                        logger.warning(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âš ï¸ ä»æ•°æ®åº“è·å–å…¬å¸åå¤±è´¥ï¼Œä½¿ç”¨è‚¡ç¥¨ä»£ç : {clean_ticker}ï¼Œé”™è¯¯: {e}")
                        real_name = clean_ticker
                    
                    # ä½¿ç”¨ real_name æ›¿ä»£åŸæ¥çš„ stock_name å˜é‡
                    stock_name = real_name
                    
                    if not stock_name or stock_name == ticker:
                        # å°è¯•è·å–ä¸­æ–‡åç§°
                        try:
                            if is_hk:
                                from tradingagents.dataflows.interface import get_hk_stock_info_unified
                                info = get_hk_stock_info_unified(ticker)
                            else:
                                from tradingagents.dataflows.interface import get_china_stock_info_unified
                                info = get_china_stock_info_unified(ticker)
                            
                            if isinstance(info, dict) and 'name' in info:
                                stock_name = info['name']
                        except:
                            pass
                    
                    # å†æ¬¡æ¸…ç†åç§° (é˜²æ­¢å¸¦æœ‰"æ¸¯è‚¡"å‰ç¼€)
                    if stock_name:
                        stock_name = stock_name.replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                        # å»é™¤å¸¸è§çš„åç¼€å’Œå†—ä½™è¯ (å¦‚ï¼šäº¬ä¸œé›†å›¢-SW -> äº¬ä¸œ)
                        # 1. å…ˆè¿›è¡ŒNFKCæ ‡å‡†åŒ–ï¼Œå°†å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’
                        import unicodedata
                        stock_name = unicodedata.normalize('NFKC', stock_name)
                        # 2. å»é™¤é›†å›¢ã€è‚¡ä»½ç­‰åç¼€ï¼Œä»¥åŠ -SW, -W ç­‰åç¼€
                        stock_name = re.sub(r'(é›†å›¢|è‚¡ä»½|æœ‰é™å…¬å¸|ï¼.*|-.*|\(.*\)|ï¼ˆ.*ï¼‰)', '', stock_name)
                        stock_name = stock_name.strip()
                    
                    # æ„é€ ä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢ (å…¨ç½‘æœç´¢ï¼Œæ›´å®½æ¾çš„å…³é”®è¯)
                    # ç­–ç•¥ï¼šä½¿ç”¨å…¬å¸å+æƒ…ç»ªç›¸å…³å…³é”®è¯ï¼Œä¸é™å®šç‰¹å®šç½‘ç«™
                    search_queries = []
                    
                    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šå…¬å¸å + æŠ•èµ„è®¨è®ºå…³é”®è¯ (å…¨ç½‘)
                    if stock_name and stock_name != ticker:
                        search_queries.append(f'{stock_name} æŠ•èµ„ åˆ†æ è®¨è®º')
                        search_queries.append(f'{stock_name} è‚¡ç¥¨ è§‚ç‚¹ è¯„è®º')
                    
                    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šè‚¡ç¥¨ä»£ç  + å…³é”®è¯
                    search_queries.append(f'{clean_ticker} è‚¡ç¥¨ åˆ†æ è§‚ç‚¹')
                    
                    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå…¬å¸å + é›ªçƒ/è‚¡å§ (ä½œä¸ºfallback)
                    if stock_name and stock_name != ticker:
                        search_queries.append(f'{stock_name} site:xueqiu.com OR site:guba.eastmoney.com')
                    
                    logger.info(f"ğŸ” [Serper] å‡†å¤‡æ‰§è¡Œ {len(search_queries)} ä¸ªæœç´¢ç­–ç•¥")
                    
                    url = "https://google.serper.dev/search"
                    headers = {
                        'X-API-KEY': serper_api_key,
                        'Content-Type': 'application/json'
                    }
                    
                    def perform_search(query, time_range="qdr:w"):
                        """æ‰§è¡Œæœç´¢ï¼Œé»˜è®¤è¿‡å»ä¸€å‘¨"""
                        payload = json.dumps({
                            "q": query,
                            "tbs": time_range,
                            "num": 15
                        })
                        response = requests.request("POST", url, headers=headers, data=payload, timeout=10)
                        return response.json().get('organic', [])
                    
                    organic_results = []
                    successful_query = None
                    
                    # ä¾æ¬¡å°è¯•å„ä¸ªæŸ¥è¯¢ç­–ç•¥
                    for query in search_queries:
                        logger.info(f"ğŸ” [Serper] å°è¯•æŸ¥è¯¢: {query}")
                        results = perform_search(query)
                        if results:
                            organic_results = results
                            successful_query = query
                            logger.info(f"âœ… [Serper] æŸ¥è¯¢æˆåŠŸï¼Œè·å– {len(results)} æ¡ç»“æœ")
                            break
                        else:
                            logger.warning(f"âš ï¸ [Serper] æŸ¥è¯¢æ— ç»“æœ: {query}")
                    
                    # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾çš„æŸ¥è¯¢ (è¿‡å»ä¸€ä¸ªæœˆ)
                    if not organic_results and stock_name:
                        logger.warning(f"âš ï¸ [Serper] æ‰€æœ‰ç­–ç•¥å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾æŸ¥è¯¢ (è¿‡å»ä¸€ä¸ªæœˆ)")
                        fallback_query = f'{stock_name} è‚¡ç¥¨'
                        organic_results = perform_search(fallback_query, "qdr:m")
                        if organic_results:
                            successful_query = fallback_query
                            logger.info(f"âœ… [Serper] å®½æ¾æŸ¥è¯¢æˆåŠŸï¼Œè·å– {len(organic_results)} æ¡ç»“æœ")
                    
                    if organic_results:
                        discussions = []
                        for item in organic_results:
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            source = item.get('link', '')
                            
                            # æ™ºèƒ½è¯†åˆ«æ¥æºå¹³å°
                            platform = "ç½‘ç»œ"
                            if "xueqiu.com" in source:
                                platform = "é›ªçƒ"
                            elif "eastmoney.com" in source or "guba" in source:
                                platform = "è‚¡å§"
                            elif "sina.com" in source:
                                platform = "æ–°æµª"
                            elif "163.com" in source:
                                platform = "ç½‘æ˜“"
                            elif "qq.com" in source:
                                platform = "è…¾è®¯"
                            elif "baidu.com" in source or "baijiahao" in source:
                                platform = "ç™¾åº¦"
                            elif "zhihu.com" in source:
                                platform = "çŸ¥ä¹"
                            elif "toutiao.com" in source:
                                platform = "å¤´æ¡"
                            elif "weibo.com" in source:
                                platform = "å¾®åš"
                            elif "36kr.com" in source:
                                platform = "36æ°ª"
                            elif "wallstreetcn.com" in source:
                                platform = "åå°”è¡—è§é—»"
                            elif "cls.cn" in source:
                                platform = "è´¢è”ç¤¾"
                                
                            discussions.append(f"- [{platform}] **{title}**: {snippet}")
                        
                        discussion_text = "\n".join(discussions)
                        
                        sentiment_summary = f"""
## å¸‚åœºæƒ…ç»ªä¸æŠ•èµ„è€…è§‚ç‚¹åˆ†æ (Serperå…¨ç½‘æœç´¢)

**è‚¡ç¥¨**: {ticker} ({stock_name})
**åˆ†ææ—¥æœŸ**: {curr_date}
**æœç´¢ç­–ç•¥**: {successful_query}
**ç»“æœæ•°é‡**: {len(organic_results)} æ¡

### æŠ•èµ„è€…è®¨è®ºä¸è§‚ç‚¹
{discussion_text}

### æƒ…ç»ªåˆ†æè¦æ±‚
è¯·åŸºäºä¸Šè¿°æœç´¢ç»“æœï¼Œåˆ†æï¼š
1. æŠ•èµ„è€…æ•´ä½“æƒ…ç»ªå€¾å‘ï¼ˆä¹è§‚/æ‚²è§‚/ä¸­æ€§ï¼‰
2. ä¸»è¦å…³æ³¨ç‚¹å’Œè®¨è®ºè¯é¢˜
3. æ½œåœ¨çš„é£é™©ç‚¹æˆ–åˆ©å¥½å› ç´ 
"""
                        result_data.append(sentiment_summary)
                        response_dict["content"] = sentiment_summary
                        response_dict["status"] = "success"
                        response_dict["source"] = "Serper/Googleå…¨ç½‘æœç´¢"
                        logger.info(f"âœ… [Serper] æˆåŠŸè·å– {len(organic_results)} æ¡æŠ•èµ„è€…è®¨è®º")
                    else:
                        # ğŸ”¥ é™çº§æ–¹æ¡ˆï¼šSerper æ— ç»“æœæ—¶ï¼Œè°ƒç”¨æ–°é—»å·¥å…·è¡¥ä½
                        logger.warning(f"âš ï¸ [Serper] æœä¸åˆ°ç¤¾äº¤åª’ä½“æ•°æ®ï¼Œå°è¯•è°ƒç”¨æ–°é—»å·¥å…·è¡¥ä½...")
                        
                        # è°ƒç”¨ get_stock_news_unified è·å–æœ€æ–°æ–°é—»
                        news_res = self.get_stock_news_unified(ticker, max_news=20)
                        news_content = news_res.get("content", "")
                        
                        if news_content and "æ— æ³•è·å–" not in news_content:
                            sentiment_summary = f"""
## å¸‚åœºæƒ…ç»ªä¸èˆ†æƒ…åˆ†æ (ç”±äºç¤¾äº¤åª’ä½“æ•°æ®å—é™ï¼Œå·²åˆ‡æ¢è‡³æ·±åº¦æ–°é—»èˆ†æƒ…æ¨¡å¼)

**è‚¡ç¥¨**: {ticker} ({stock_name})
**åˆ†ææ—¥æœŸ**: {curr_date}
**è¡¥ä½æº**: æ ¸å¿ƒå…¬å‘Šä¸ä¸»æµè´¢ç»æ–°é—»

### æ ¸å¿ƒèˆ†æƒ…å†…å®¹
{news_content}

### æƒ…ç»ªåˆ¤ç ”æ‰§è¡ŒæŒ‡ä»¤
ç”±äºå½“å‰ç¤¾äº¤åª’ä½“ï¼ˆé›ªçƒ/è‚¡å§ï¼‰å®æ—¶è®¨è®ºæŠ“å–å—é™ï¼Œè¯·ä½ ä½œä¸ºåˆ†æå¸ˆï¼š
1. **ä»¥å…¬å‘Šä¸ºå‡†**ï¼šé‡ç‚¹ç ”è¯»å…¬å‘Šï¼ˆNoticeï¼‰ä¸­çš„å›è´­ã€è‚¡æƒå˜åŠ¨ç­‰å®é”¤ä¿¡æ¯ã€‚
2. **èˆ†æƒ…æ¨æµ‹**ï¼šé€šè¿‡ä¸»æµåª’ä½“ï¼ˆè¯åˆ¸æ—¶æŠ¥ã€è´¢è”ç¤¾ç­‰ï¼‰çš„æŠ¥é“åŸºè°ƒï¼Œæ¨æ–­æœºæ„å’Œæ•£æˆ·çš„å¸‚åœºé¢„æœŸã€‚
3. **æƒ…ç»ªæ˜ å°„**ï¼šåŸºäºä¸šåŠ¡è¿›å±•ï¼ˆcmsArticleWebOldï¼‰å’Œèµ„é‡‘æµå‘ï¼Œåˆ»ç”»å½“å‰å¸‚åœºå¯¹è¯¥æ ‡çš„çš„å†·çƒ­ç¨‹åº¦ã€‚
"""
                            result_data.append(sentiment_summary)
                            response_dict["content"] = sentiment_summary
                            response_dict["status"] = "warning"
                            response_dict["source"] = "æ–°é—»èˆ†æƒ…è¡¥ä½"
                            logger.info(f"âœ… [æƒ…ç»ªè¡¥ä½] å·²é€šè¿‡æ–°é—»æºæˆåŠŸè¡¥ä½ï¼Œé•¿åº¦: {len(news_content)}")
                        else:
                            logger.warning(f"âš ï¸ [Serper] ä¸”æ–°é—»æºäº¦æ— æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼")
                            response_dict["summary"] = f"å½“å‰æš‚æ— è‚¡ç¥¨ {ticker} ({stock_name}) çš„è¯¦ç»†å¸‚åœºæƒ…ç»ªæ•°æ®ï¼Œç³»ç»Ÿé»˜è®¤ç»™äºˆä¸­æ€§è¯„çº§ã€‚è¯·ä»¥å¸‚åœºå®¢è§‚æŒ‡æ ‡ä¸ºå‡†ã€‚"
                            response_dict["company_name_check"] = f"è¯·ä¸¥æ ¼åŸºäºè‚¡ç¥¨ä»£ç  {ticker} ç¡®è®¤å…¬å¸åç§°ï¼Œç¦æ­¢è‡†æµ‹"
                            return response_dict


                except Exception as e:
                    # ğŸ”¥ æ•è· Serper å¼‚å¸¸ï¼ˆå¦‚ Key ç¼ºå¤±ï¼‰ï¼ŒåŒæ ·å°è¯•æ–°é—»è¡¥ä½
                    logger.error(f"âŒ [Serper] æœç´¢å¤±è´¥: {e}ï¼Œå°è¯•æ‰§è¡Œæ–°é—»é™çº§æ–¹æ¡ˆ...")
                    
                    try:
                        news_res = self.get_stock_news_unified(ticker, max_news=20)
                        news_content = news_res.get("content", "")
                        
                        if news_content and "æ— æ³•è·å–" not in news_content:
                             sentiment_summary = f"""
## èˆ†æƒ…çƒ­åº¦åˆ†ææŠ¥å‘Š (å¤‡ç”¨æ¨¡å¼)

**åˆ†æå¯¹è±¡**: {ticker}
**åˆ†ææ—¥æœŸ**: {curr_date}
**æ•°æ®æ¥æº**: å®æ—¶è´¢ç»æ–°é—»ä¸å®˜æ–¹å…¬å‘Š

{news_content}

---
**åˆ†æå¸ˆæç¤º**ï¼šå½“å‰ç¤¾äº¤å¹³å°æ•°æ®æ¥å£ç»´æŠ¤ä¸­ï¼Œè¯·åŸºäºä¸Šè¿°æƒå¨æ–°é—»å’Œå…¬å‘Šå†…å®¹ï¼Œç ”åˆ¤æŠ•èµ„è€…çš„å¿ƒç†é¢„æœŸå’Œå¸‚åœºåšå¼ˆå½¢æ€ã€‚
"""
                             result_data.append(sentiment_summary)
                             response_dict["content"] = sentiment_summary
                             response_dict["status"] = "warning"
                             response_dict["source"] = "æ–°é—»èˆ†æƒ…é™çº§"
                             return response_dict
                    except Exception as nested_e:
                        logger.error(f"âŒ [é™çº§å¤±è´¥] æ–°é—»è¡¥ä½ä¹ŸæŒ‚äº†: {nested_e}")

                    response_dict["summary"] = f"ã€ç³»ç»Ÿæç¤ºã€‘æƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}"
                    response_dict["status"] = "error"
                    response_dict["error"] = str(e)
                    return response_dict


            else:
                # ç¾è‚¡ï¼šä½¿ç”¨Redditæƒ…ç»ªåˆ†æ
                logger.info(f"ğŸ‡ºğŸ‡¸ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] å¤„ç†ç¾è‚¡æƒ…ç»ª...")

                try:
                    from tradingagents.dataflows.interface import get_reddit_sentiment

                    sentiment_data = get_reddit_sentiment(ticker, curr_date)
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\n{sentiment_data}")
                    response_dict["content"] = sentiment_data
                    response_dict["source"] = "Reddit"
                    response_dict["status"] = "success"
                except Exception as e:
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}")
                    response_dict["content"] = f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}"
                    response_dict["status"] = "error"

            # ç»„åˆæ‰€æœ‰æ•°æ®
            combined_result = f"""# {ticker} æƒ…ç»ªåˆ†æ

**è‚¡ç¥¨ç±»å‹**: {market_info['market_name']}
**åˆ†ææ—¥æœŸ**: {curr_date}

{chr(10).join(result_data)}

---
*æ•°æ®æ¥æº: Serper (Google Search) / Reddit*
"""
            response_dict["content"] = combined_result
            
            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] æ•°æ®è·å–å®Œæˆï¼Œæ€»é•¿åº¦: {len(combined_result)}")
            return response_dict

        except Exception as e:
            error_msg = f"ç»Ÿä¸€æƒ…ç»ªåˆ†æå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] {error_msg}")
            return {
                "status": "error",
                "error": str(e),
                "ticker": ticker,
                "content": error_msg
            }

    def _get_us_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–ç¾è‚¡æ–°é—»"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è·å–ç¾è‚¡ {stock_code} æ–°é—»")
        
        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        
        # ä¼˜å…ˆçº§1: OpenAIå…¨çƒæ–°é—»
        try:
            if hasattr(self.toolkit, 'get_global_news_openai'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•OpenAIç¾è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_global_news_openai.invoke({"curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… OpenAIç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "OpenAIç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] OpenAIç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§2: Googleæ–°é—»ï¼ˆè‹±æ–‡æœç´¢ï¼‰
        try:
            if hasattr(self.toolkit, 'get_google_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•Googleç¾è‚¡æ–°é—»...")
                query = f"{stock_code} stock news earnings financial"
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "Googleç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Googleç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        # ä¼˜å…ˆçº§3: FinnHubæ–°é—»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            if hasattr(self.toolkit, 'get_finnhub_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å°è¯•FinnHubç¾è‚¡æ–°é—»...")
                # ä½¿ç”¨LangChainå·¥å…·çš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼š.invoke()æ–¹æ³•å’Œå­—å…¸å‚æ•°
                result = self.toolkit.get_finnhub_news.invoke({"symbol": stock_code, "max_results": min(max_news, 50)})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… FinnHubç¾è‚¡æ–°é—»è·å–æˆåŠŸ: {len(result)} å­—ç¬¦")
                    return self._format_news_result(result, "FinnHubç¾è‚¡æ–°é—»", model_info)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] FinnHubç¾è‚¡æ–°é—»è·å–å¤±è´¥: {e}")
        
        return "âŒ æ— æ³•è·å–ç¾è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
    
    def _format_news_result(self, news_content: str, source: str, model_info: str = "") -> str:
        """æ ¼å¼åŒ–æ–°é—»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ğŸ” æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°åŸå§‹æ–°é—»å†…å®¹
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ åŸå§‹æ–°é—»å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦): {news_content[:500]}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š åŸå§‹å†…å®¹é•¿åº¦: {len(news_content)} å­—ç¬¦")
        
        # æ£€æµ‹æ˜¯å¦ä¸ºGoogle/Geminiæ¨¡å‹
        is_google_model = any(keyword in model_info.lower() for keyword in ['google', 'gemini', 'gemma'])
        original_length = len(news_content)
        google_control_applied = False
        
        # ğŸ” æ·»åŠ Googleæ¨¡å‹æ£€æµ‹æ—¥å¿—
        if is_google_model:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†")
        
        # å¯¹Googleæ¨¡å‹è¿›è¡Œç‰¹æ®Šçš„é•¿åº¦æ§åˆ¶
        if is_google_model and len(news_content) > 5000:  # é™ä½é˜ˆå€¼åˆ°5000å­—ç¬¦
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œæ–°é—»å†…å®¹è¿‡é•¿({len(news_content)}å­—ç¬¦)ï¼Œè¿›è¡Œé•¿åº¦æ§åˆ¶...")
            
            # æ›´ä¸¥æ ¼çš„é•¿åº¦æ§åˆ¶ç­–ç•¥
            lines = news_content.split('\n')
            important_lines = []
            char_count = 0
            target_length = 3000  # ç›®æ ‡é•¿åº¦è®¾ä¸º3000å­—ç¬¦
            
            # ç¬¬ä¸€è½®ï¼šä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®è¯çš„é‡è¦è¡Œ
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
                important_keywords = ['è‚¡ç¥¨', 'å…¬å¸', 'è´¢æŠ¥', 'ä¸šç»©', 'æ¶¨è·Œ', 'ä»·æ ¼', 'å¸‚å€¼', 'è¥æ”¶', 'åˆ©æ¶¦', 
                                    'å¢é•¿', 'ä¸‹è·Œ', 'ä¸Šæ¶¨', 'ç›ˆåˆ©', 'äºæŸ', 'æŠ•èµ„', 'åˆ†æ', 'é¢„æœŸ', 'å…¬å‘Š']
                
                is_important = any(keyword in line for keyword in important_keywords)
                
                if is_important and char_count + len(line) < target_length:
                    important_lines.append(line)
                    char_count += len(line)
                elif not is_important and char_count + len(line) < target_length * 0.7:  # éé‡è¦å†…å®¹æ›´ä¸¥æ ¼é™åˆ¶
                    important_lines.append(line)
                    char_count += len(line)
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡é•¿åº¦ï¼Œåœæ­¢æ·»åŠ 
                if char_count >= target_length:
                    break
            
            # å¦‚æœæå–çš„é‡è¦å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œè¿›ä¸€æ­¥æˆªæ–­
            if important_lines:
                processed_content = '\n'.join(important_lines)
                if len(processed_content) > target_length:
                    processed_content = processed_content[:target_length] + "...(å†…å®¹å·²æ™ºèƒ½æˆªæ–­)"
                
                news_content = processed_content
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ¨¡å‹æ™ºèƒ½é•¿åº¦æ§åˆ¶å®Œæˆï¼Œä»{original_length}å­—ç¬¦å‹ç¼©è‡³{len(news_content)}å­—ç¬¦")
            else:
                # å¦‚æœæ²¡æœ‰é‡è¦è¡Œï¼Œç›´æ¥æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                news_content = news_content[:target_length] + "...(å†…å®¹å·²å¼ºåˆ¶æˆªæ–­)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Googleæ¨¡å‹å¼ºåˆ¶æˆªæ–­è‡³{target_length}å­—ç¬¦")
        
        # è®¡ç®—æœ€ç»ˆçš„æ ¼å¼åŒ–ç»“æœé•¿åº¦ï¼Œç¡®ä¿æ€»é•¿åº¦åˆç†
        base_format_length = 300  # æ ¼å¼åŒ–æ¨¡æ¿çš„å¤§æ¦‚é•¿åº¦
        if is_google_model and (len(news_content) + base_format_length) > 4000:
            # å¦‚æœåŠ ä¸Šæ ¼å¼åŒ–åä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ–°é—»å†…å®¹
            max_content_length = 3500
            if len(news_content) > max_content_length:
                news_content = news_content[:max_content_length] + "...(å·²ä¼˜åŒ–é•¿åº¦)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ Googleæ¨¡å‹æœ€ç»ˆé•¿åº¦ä¼˜åŒ–ï¼Œå†…å®¹é•¿åº¦: {len(news_content)}å­—ç¬¦")
        
        formatted_result = f"""
=== ğŸ“° æ–°é—»æ•°æ®æ¥æº: {source} ===
è·å–æ—¶é—´: {timestamp}
æ•°æ®é•¿åº¦: {len(news_content)} å­—ç¬¦
{f"æ¨¡å‹ç±»å‹: {model_info}" if model_info else ""}
{f"ğŸ”§ Googleæ¨¡å‹é•¿åº¦æ§åˆ¶å·²åº”ç”¨ (åŸé•¿åº¦: {original_length} å­—ç¬¦)" if google_control_applied else ""}

=== ğŸ“‹ æ–°é—»å†…å®¹ ===
{news_content}

=== âœ… æ•°æ®çŠ¶æ€ ===
çŠ¶æ€: æˆåŠŸè·å–
æ¥æº: {source}
æ—¶é—´æˆ³: {timestamp}
"""
        return formatted_result.strip()


def create_unified_news_tool(toolkit):
    """åˆ›å»ºç»Ÿä¸€æ–°é—»å·¥å…·å‡½æ•°"""
    analyzer = UnifiedNewsAnalyzer(toolkit)
    
    def get_stock_news_unified(stock_code: str, max_news: int = 100, model_info: str = ""):
        """
        ç»Ÿä¸€æ–°é—»è·å–å·¥å…·
        
        Args:
            stock_code (str): è‚¡ç¥¨ä»£ç  (æ”¯æŒAè‚¡å¦‚000001ã€æ¸¯è‚¡å¦‚0700.HKã€ç¾è‚¡å¦‚AAPL)
            max_news (int): æœ€å¤§æ–°é—»æ•°é‡ï¼Œé»˜è®¤100
            model_info (str): å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
        """
        

        if not stock_code:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
        
        return analyzer.get_stock_news_unified(stock_code, max_news, model_info)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_news_unified.name = "get_stock_news_unified"
    get_stock_news_unified.description = """
ç»Ÿä¸€æ–°é—»è·å–å·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æ–°é—»

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ ¹æ®è‚¡ç¥¨ç±»å‹é€‰æ‹©æœ€ä½³æ–°é—»æº
- Aè‚¡: ä¼˜å…ˆä¸œæ–¹è´¢å¯Œ -> Googleä¸­æ–‡ -> OpenAI
- æ¸¯è‚¡: ä¼˜å…ˆGoogle -> OpenAI -> å®æ—¶æ–°é—»
- ç¾è‚¡: ä¼˜å…ˆOpenAI -> Googleè‹±æ–‡ -> FinnHub
- è¿”å›æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹
- æ”¯æŒGoogleæ¨¡å‹çš„ç‰¹æ®Šé•¿åº¦æ§åˆ¶
"""
    
    def get_stock_sentiment_unified(ticker: str, curr_date: str) -> str:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            curr_date: å½“å‰æ—¥æœŸ
            
        """
        if not ticker:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
            
        return analyzer.get_stock_sentiment_unified(ticker, curr_date)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_sentiment_unified.name = "get_stock_sentiment_unified"
    get_stock_sentiment_unified.description = """
ç»Ÿä¸€è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æƒ…ç»ªæ•°æ®

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ¸¯è‚¡/Aè‚¡: ä½¿ç”¨Serperæœç´¢é›ªçƒ/è‚¡å§çš„æ•£æˆ·è®¨è®º
- ç¾è‚¡: ä½¿ç”¨Reddit/Twitteræƒ…ç»ªæ•°æ®
- è¿”å›æ ¼å¼åŒ–çš„æƒ…ç»ªåˆ†ææŠ¥å‘Š
"""

    return get_stock_news_unified, get_stock_sentiment_unified