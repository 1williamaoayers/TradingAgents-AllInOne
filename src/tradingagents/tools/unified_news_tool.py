#!/usr/bin/env python3
import json
import requests
"""
ç»Ÿä¸€æ–°é—»åˆ†æå·¥å…·
æ•´åˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ç­‰ä¸åŒå¸‚åœºçš„æ–°é—»è·å–é€»è¾‘åˆ°ä¸€ä¸ªå·¥å…·å‡½æ•°ä¸­
è®©å¤§æ¨¡å‹åªéœ€è¦è°ƒç”¨ä¸€ä¸ªå·¥å…·å°±èƒ½è·å–æ‰€æœ‰ç±»å‹è‚¡ç¥¨çš„æ–°é—»æ•°æ®
"""

import logging
from datetime import datetime
import re
import asyncio
from concurrent.futures import ThreadPoolExecutor

# å°è¯•å¯¼å…¥ScraperAdapter
try:
    from app.worker.news_adapters.scraper_adapter import ScraperAdapter
except ImportError:
    # å°è¯•å¤‡ç”¨è·¯å¾„
    try:
        from tradingagents.dataflows.interface import ScraperAdapter
    except ImportError:
        ScraperAdapter = None

logger = logging.getLogger(__name__)


# ==================== æ™ºèƒ½å»é‡å·¥å…· ====================

def normalize_title(title: str) -> str:
    """
    æ ‡å‡†åŒ–æ–°é—»æ ‡é¢˜ï¼Œç”¨äºå»é‡æ¯”è¾ƒ
    å¤„ç†ï¼šæ•°å­—æ ¼å¼å·®å¼‚ã€ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·ç­‰
    
    Args:
        title: åŸå§‹æ ‡é¢˜
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ ‡é¢˜
    """
    if not title:
        return ""
    
    # 1. è½¬å°å†™
    normalized = title.lower()
    
    # 2. ç§»é™¤æ‰€æœ‰ç©ºæ ¼
    normalized = re.sub(r'\s+', '', normalized)
    
    # 3. æ ‡å‡†åŒ–æ•°å­—æ ¼å¼ï¼ˆ1.00äº¿ -> 1äº¿ï¼Œ1,000 -> 1000ï¼‰
    normalized = re.sub(r'\.0+', '', normalized)  # 1.00 -> 1
    normalized = re.sub(r',', '', normalized)      # 1,000 -> 1000
    
    # 4. ç§»é™¤å¸¸è§æ ‡ç‚¹
    normalized = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''ã€ã€‘\[\]()ï¼ˆï¼‰]', '', normalized)
    
    # 5. ç§»é™¤æ¥æºæ ‡è¯†ï¼ˆå¦‚ [è¯åˆ¸æ—¶æŠ¥]ã€ã€è´¢è”ç¤¾ã€‘ï¼‰
    normalized = re.sub(r'\[.*?\]', '', normalized)
    normalized = re.sub(r'ã€.*?ã€‘', '', normalized)
    
    return normalized


def is_similar_title(title1: str, title2: str, threshold: float = 0.8) -> bool:
    """
    åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼ï¼ˆä½¿ç”¨ç®€å•çš„å­—ç¬¦é‡å ç‡ï¼‰
    
    Args:
        title1: æ ‡é¢˜1
        title2: æ ‡é¢˜2
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.8ï¼ˆ80%ç›¸ä¼¼å°±è®¤ä¸ºé‡å¤ï¼‰
        
    Returns:
        æ˜¯å¦ç›¸ä¼¼
    """
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    
    if not norm1 or not norm2:
        return False
    
    # å®Œå…¨ç›¸åŒ
    if norm1 == norm2:
        return True
    
    # ä¸€ä¸ªåŒ…å«å¦ä¸€ä¸ªï¼ˆå­ä¸²åŒ¹é…ï¼‰
    if norm1 in norm2 or norm2 in norm1:
        return True
    
    # å­—ç¬¦é‡å ç‡è®¡ç®—
    set1 = set(norm1)
    set2 = set(norm2)
    
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    
    if union == 0:
        return False
    
    similarity = intersection / union
    return similarity >= threshold


def extract_date_from_title(title: str) -> str:
    """
    ä»æ ‡é¢˜ä¸­æå–æ—¥æœŸä¿¡æ¯
    æ”¯æŒæ ¼å¼ï¼š12æœˆ15æ—¥ã€12-15ã€12/15ã€2024å¹´12æœˆ15æ—¥ç­‰
    
    Args:
        title: æ–°é—»æ ‡é¢˜
        
    Returns:
        æå–åˆ°çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚æœæ²¡æœ‰æ—¥æœŸåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not title:
        return ""
    
    # æ¨¡å¼1: XæœˆXæ—¥
    match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', title)
    if match:
        return f"{match.group(1)}-{match.group(2)}"
    
    # æ¨¡å¼2: Xå¹´XæœˆXæ—¥
    match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', title)
    if match:
        return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
    
    # æ¨¡å¼3: YYYY-MM-DD æˆ– MM-DD
    match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', title)
    if match:
        return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
    
    match = re.search(r'(\d{1,2})-(\d{1,2})', title)
    if match:
        return f"{match.group(1)}-{match.group(2)}"
    
    return ""


class NewsDeduplicator:
    """
    ğŸ”¥ ç»ˆææ–°é—»å»é‡å™¨
    
    ç‰¹æ€§ï¼š
    1. æ—¥æœŸæ•æ„Ÿï¼šä¸åŒæ—¥æœŸçš„ç›¸ä¼¼æ–°é—»ä¸ä¼šè¢«å»é‡
    2. ä¿ç•™æœ€å®Œæ•´ç‰ˆæœ¬ï¼šç›¸ä¼¼æ—¶ä¿ç•™æ›´é•¿çš„æ ‡é¢˜
    3. æ ‡å‡†åŒ–å¤„ç†ï¼šç»Ÿä¸€æ•°å­—æ ¼å¼ã€å»æ ‡ç‚¹ç­‰
    """
    
    def __init__(self):
        # å­˜å‚¨æ ¼å¼: {normalized_title: (original_title, date)}
        self.seen_items = {}
        self.stats = {"total": 0, "duplicates": 0, "replaced": 0}
    
    def check_and_add(self, title: str, threshold: float = 0.75) -> bool:
        """
        ğŸ”¥ ç»ˆæå»é‡é€»è¾‘
        
        Args:
            title: å¾…æ£€æŸ¥çš„æ ‡é¢˜
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            True = åº”è¯¥ä¿ç•™ï¼ˆæ–°æ ‡é¢˜æˆ–æ›´å®Œæ•´ç‰ˆæœ¬ï¼‰
            False = åº”è¯¥è·³è¿‡ï¼ˆé‡å¤ä¸”ä¸å¦‚å·²æœ‰ç‰ˆæœ¬å®Œæ•´ï¼‰
        """
        if not title or len(title.strip()) < 5:
            return False
        
        self.stats["total"] += 1
        
        # 1. æå–æ—¥æœŸ
        new_date = extract_date_from_title(title)
        normalized = normalize_title(title)
        
        # 2. æ£€æŸ¥å®Œå…¨åŒ¹é…ï¼ˆæ ‡å‡†åŒ–åç›¸åŒï¼‰
        if normalized in self.seen_items:
            old_title, old_date = self.seen_items[normalized]
            
            # æ—¥æœŸä¸åŒ = ä¸åŒäº‹ä»¶ï¼Œéƒ½ä¿ç•™
            if new_date and old_date and new_date != old_date:
                # ç”¨ä¸åŒçš„keyå­˜å‚¨ï¼ˆåŠ æ—¥æœŸåç¼€ï¼‰
                self.seen_items[f"{normalized}_{new_date}"] = (title, new_date)
                return True
            
            # æ—¥æœŸç›¸åŒæˆ–æ— æ—¥æœŸï¼Œæ¯”è¾ƒé•¿åº¦
            if len(title) > len(old_title):
                # æ–°æ ‡é¢˜æ›´é•¿ï¼Œæ›¿æ¢
                self.seen_items[normalized] = (title, new_date or old_date)
                self.stats["replaced"] += 1
                return True
            else:
                # æ—§æ ‡é¢˜æ›´é•¿æˆ–ç›¸ç­‰ï¼Œè·³è¿‡
                self.stats["duplicates"] += 1
                return False
        
        # 3. æ£€æŸ¥ç›¸ä¼¼åŒ¹é…
        for norm_key, (old_title, old_date) in list(self.seen_items.items()):
            if is_similar_title(title, old_title, threshold):
                # æ—¥æœŸä¸åŒ = ä¸åŒäº‹ä»¶
                if new_date and old_date and new_date != old_date:
                    continue  # ä¸ç®—é‡å¤ï¼Œç»§ç»­æ£€æŸ¥
                
                # æ—¥æœŸç›¸åŒæˆ–æ— æ—¥æœŸï¼Œæ¯”è¾ƒé•¿åº¦
                if len(title) > len(old_title):
                    # æ–°æ ‡é¢˜æ›´é•¿ï¼Œæ›¿æ¢
                    del self.seen_items[norm_key]
                    self.seen_items[normalized] = (title, new_date or old_date)
                    self.stats["replaced"] += 1
                    return True
                else:
                    # æ—§æ ‡é¢˜æ›´é•¿æˆ–ç›¸ç­‰ï¼Œè·³è¿‡
                    self.stats["duplicates"] += 1
                    return False
        
        # 4. å…¨æ–°æ ‡é¢˜ï¼Œæ·»åŠ 
        self.seen_items[normalized] = (title, new_date)
        return True
    
    def get_stats(self) -> dict:
        """è·å–å»é‡ç»Ÿè®¡"""
        return {
            "total_checked": self.stats["total"],
            "duplicates_removed": self.stats["duplicates"],
            "replaced_with_longer": self.stats["replaced"],
            "unique_kept": len(self.seen_items)
        }

class UnifiedNewsAnalyzer:
    """ç»Ÿä¸€æ–°é—»åˆ†æå™¨ï¼Œæ•´åˆæ‰€æœ‰æ–°é—»è·å–é€»è¾‘"""
    
    def __init__(self, toolkit):
        """åˆå§‹åŒ–ç»Ÿä¸€æ–°é—»åˆ†æå™¨
        
        Args:
            toolkit: åŒ…å«å„ç§æ–°é—»è·å–å·¥å…·çš„å·¥å…·åŒ…
        """
        self.toolkit = toolkit
        
    def get_stock_news_unified(self, stock_code: str, max_news: int = 10, model_info: str = "") -> dict:
        """
        ç»Ÿä¸€æ–°é—»è·å–æ¥å£
        æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹å¹¶è·å–ç›¸åº”æ–°é—»
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡
            model_info: å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
            
        Returns:
            dict: åŒ…å«æ–°é—»å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] å¼€å§‹è·å– {stock_code} çš„æ–°é—»ï¼Œæ¨¡å‹: {model_info}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– å½“å‰æ¨¡å‹ä¿¡æ¯: {model_info}")
        
        # è¯†åˆ«è‚¡ç¥¨ç±»å‹
        stock_type = self._identify_stock_type(stock_code)
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ç±»å‹: {stock_type}")
        
        # æ ¹æ®è‚¡ç¥¨ç±»å‹è°ƒç”¨ç›¸åº”çš„è·å–æ–¹æ³•
        if stock_type == "Aè‚¡":
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        elif stock_type == "æ¸¯è‚¡":
            result_str = self._get_hk_share_news(stock_code, max_news, model_info)
        elif stock_type == "ç¾è‚¡":
            result_str = self._get_us_share_news(stock_code, max_news, model_info)
        else:
            # é»˜è®¤ä½¿ç”¨Aè‚¡é€»è¾‘
            result_str = self._get_a_share_news(stock_code, max_news, model_info)
        
        # ğŸ” æ„å»ºâ€œæ»¡è¡€ç‰ˆâ€æ•°æ®é›†ï¼šæ³¨å…¥å—å‘/åŒ—å‘èµ„é‡‘æµæ•°æ® (ä»…é™Aè‚¡/æ¸¯è‚¡)
        fund_flow_str = ""
        if stock_type in ["Aè‚¡", "æ¸¯è‚¡"]:
            try:
                from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
                provider = get_akshare_provider()
                # ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„èµ„é‡‘æµè·å–
                fund_flow_str = provider.get_hsgt_fund_flow_sync(stock_code)
                
                if fund_flow_str and result_str:  # ç¡®ä¿ä¸¤è€…éƒ½ä¸ä¸ºNone
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æˆåŠŸæ³¨å…¥èµ„é‡‘æµæ•°æ®")
                    result_str = fund_flow_str + "\n\n" + result_str
            except Exception as e:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ èµ„é‡‘æµæ³¨å…¥å¤±è´¥: {e}")

        # ğŸ” æ·»åŠ è¯¦ç»†çš„ç»“æœè°ƒè¯•æ—¥å¿—
        if result_str:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æ»¡è¡€ç‰ˆæ•°æ®é›†æ„å»ºå®Œæˆï¼Œç»“æœé•¿åº¦: {len(result_str)} å­—ç¬¦")
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ è¿”å›ç»“æœé¢„è§ˆ (å‰1000å­—ç¬¦): {result_str[:1000]}")
        else:
            logger.warning("[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ result_strä¸ºNoneï¼Œå¯èƒ½æ‰€æœ‰æ–°é—»æºéƒ½å¤±è´¥äº†")
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œè®°å½•è­¦å‘Š
        if not result_str or len(result_str.strip()) < 50:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ è¿”å›ç»“æœå¼‚å¸¸çŸ­æˆ–ä¸ºç©ºï¼")
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“ å®Œæ•´ç»“æœå†…å®¹: '{result_str}'")
        
        # æ„é€ è¿”å›å­—å…¸
        return {
            "status": "success" if result_str and len(result_str) > 50 else "warning",
            "content": result_str,
            "stock_type": stock_type,
            "ticker": stock_code,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    
    def _identify_stock_type(self, stock_code: str) -> str:
        """è¯†åˆ«è‚¡ç¥¨ç±»å‹"""
        stock_code = stock_code.upper().strip()
        
        # Aè‚¡åˆ¤æ–­
        if re.match(r'^(00|30|60|68)\d{4}$', stock_code):
            return "Aè‚¡"
        elif re.match(r'^(SZ|SH)\d{6}$', stock_code):
            return "Aè‚¡"
        
        # æ¸¯è‚¡åˆ¤æ–­
        elif re.match(r'^\d{4,5}\.HK$', stock_code):
            return "æ¸¯è‚¡"
        elif re.match(r'^\d{4,5}$', stock_code) and len(stock_code) <= 5:
            return "æ¸¯è‚¡"
        
        # ç¾è‚¡åˆ¤æ–­
        elif re.match(r'^[A-Z]{1,5}$', stock_code):
            return "ç¾è‚¡"
        elif '.' in stock_code and not stock_code.endswith('.HK'):
            return "ç¾è‚¡"
        
        # é»˜è®¤æŒ‰Aè‚¡å¤„ç†
        else:
            return "Aè‚¡"
    
    def _search_news_with_serper(self, query: str, period: str = "qdr:w") -> str:
        """
        ä½¿ç”¨Serper APIæœç´¢æ–°é—»
        Args:
            query: æœç´¢å…³é”®è¯
            period: æ—¶é—´èŒƒå›´ï¼Œé»˜è®¤ä¸ºè¿‡å»ä¸€å‘¨ (qdr:w)ï¼Œå¯é€‰ qdr:d (ä¸€å¤©), qdr:m (ä¸€æœˆ)
        """
        try:
            import os
            api_key = os.getenv("SERPER_API_KEY")
            if not api_key:
                return ""
                
            url = "https://google.serper.dev/search"
            headers = {
                'X-API-KEY': api_key,
                'Content-Type': 'application/json'
            }
            
            payload = json.dumps({
                "q": query,
                "tbs": period,
                "num": 10
            })
            
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            if response.status_code != 200:
                return ""
                
            results = response.json().get('organic', [])
            if not results:
                return ""
                
            formatted = []
            for item in results:
                title = item.get('title', '')
                snippet = item.get('snippet', '')
                link = item.get('link', '')
                date = item.get('date', '')
                formatted.append(f"### {title}\n- **æ¥æº**: {link}\n- **æ—¶é—´**: {date}\n- **æ‘˜è¦**: {snippet}\n")
                
            return "\n".join(formatted)
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Serperæœç´¢å¤±è´¥: {e}")
            return ""

    def _get_company_name_from_code(self, stock_code: str) -> str:
        """
        æ ¹æ®è‚¡ç¥¨ä»£ç è·å–å…¬å¸åç§°
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            str: å…¬å¸åç§°ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        # ç®€å•çš„æ˜ å°„è¡¨ï¼ˆå¸¸è§è‚¡ç¥¨ï¼‰
        stock_name_map = {
            '09618': 'äº¬ä¸œé›†å›¢',
            '9618': 'äº¬ä¸œé›†å›¢',
            'JD': 'äº¬ä¸œ',
            '00700': 'è…¾è®¯æ§è‚¡',
            '0700': 'è…¾è®¯æ§è‚¡',
            'BABA': 'é˜¿é‡Œå·´å·´',
            '09988': 'é˜¿é‡Œå·´å·´',
            'AAPL': 'è‹¹æœ',
            'TSLA': 'ç‰¹æ–¯æ‹‰',
            'NVDA': 'è‹±ä¼Ÿè¾¾',
            '000001': 'å¹³å®‰é“¶è¡Œ',
            '600519': 'è´µå·èŒ…å°',
        }
        
        # æ ‡å‡†åŒ–ä»£ç 
        clean_code = stock_code.replace('.HK', '').replace('.SH', '').replace('.SZ', '')
        
        # æŸ¥æ‰¾æ˜ å°„
        company_name = stock_name_map.get(clean_code, '')
        
        if company_name:
            logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ä»£ç  {stock_code} æ˜ å°„åˆ°å…¬å¸åç§°: {company_name}")
        else:
            logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] è‚¡ç¥¨ä»£ç  {stock_code} æœªæ‰¾åˆ°å…¬å¸åç§°æ˜ å°„")
        
        return company_name

    def _get_news_from_database(self, stock_code: str, max_news: int = 10, company_name: str = "") -> str:
        """
        ä»æ•°æ®åº“è·å–æ–°é—»ï¼ˆæ”¹è¿›ç‰ˆï¼šæ”¯æŒå†…å®¹ç›¸å…³æ€§æŸ¥è¯¢ï¼‰

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡
            company_name: å…¬å¸åç§°ï¼ˆç”¨äºå†…å®¹åŒ¹é…ï¼‰

        Returns:
            str: æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰æ–°é—»åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            from tradingagents.dataflows.cache.app_adapter import get_mongodb_client
            from datetime import timedelta

            # ğŸ”§ ç¡®ä¿ max_news æ˜¯æ•´æ•°ï¼ˆé˜²æ­¢ä¼ å…¥æµ®ç‚¹æ•°ï¼‰
            max_news = int(max_news)

            client = get_mongodb_client()
            if not client:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ— æ³•è¿æ¥åˆ°MongoDB")
                return ""

            db = client.get_database('tradingagents')
            collection = db.stock_news

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            # æŸ¥è¯¢æœ€è¿‘30å¤©çš„æ–°é—»ï¼ˆæ‰©å¤§æ—¶é—´èŒƒå›´ï¼‰
            thirty_days_ago = datetime.now() - timedelta(days=30)

            # ğŸ”¥ æ”¹è¿›ï¼šæ„å»ºå…³é”®è¯åˆ—è¡¨ï¼ˆæ”¯æŒå†…å®¹ç›¸å…³æ€§æŸ¥è¯¢ï¼‰
            keywords = [stock_code, clean_code]
            
            if company_name:
                # æ·»åŠ å…¬å¸åç§°ç›¸å…³å…³é”®è¯
                keywords.append(company_name)
                # å»é™¤"é›†å›¢"ã€"è‚¡ä»½"ç­‰åç¼€
                clean_name = company_name.replace('é›†å›¢', '').replace('è‚¡ä»½', '').replace('æœ‰é™å…¬å¸', '')
                if clean_name != company_name:
                    keywords.append(clean_name)
                
                # æ·»åŠ ç›¸å…³ä¸šåŠ¡å…³é”®è¯ï¼ˆé’ˆå¯¹å¤§å…¬å¸ï¼‰
                if clean_name:
                    keywords.extend([
                        f'{clean_name}ç‰©æµ',
                        f'{clean_name}é›¶å”®',
                        f'{clean_name}ç§‘æŠ€'
                    ])
            
            # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            keyword_pattern = '|'.join([k for k in keywords if k])
            
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” æŸ¥è¯¢å…³é”®è¯: {keywords[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª

            # ğŸ”¥ æ”¹è¿›ï¼šèåˆå¤šä¸ªæŸ¥è¯¢æ¡ä»¶çš„ç»“æœï¼Œä½¿ç”¨æ ‡é¢˜å»é‡
            news_items = []
            seen_titles = set()  # ç”¨äºå»é‡
            
            # å®šä¹‰æŸ¥è¯¢æ¡ä»¶ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            specific_queries = [
                # ä¼˜å…ˆçº§1: ç²¾ç¡®åŒ¹é…symbol + æ—¶é—´èŒƒå›´ï¼ˆæœ€ç›¸å…³ï¼‰
                {'symbol': clean_code, 'publish_time': {'$gte': thirty_days_ago}},
                {'symbol': stock_code, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§2: æ ‡é¢˜åŒ…å«å…³é”®è¯ + æ—¶é—´èŒƒå›´
                {'title': {'$regex': keyword_pattern, '$options': 'i'}, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§3: å†…å®¹åŒ…å«å…³é”®è¯ + æ—¶é—´èŒƒå›´
                {'content': {'$regex': keyword_pattern, '$options': 'i'}, 'publish_time': {'$gte': thirty_days_ago}},
                
                # ä¼˜å…ˆçº§4: ç²¾ç¡®åŒ¹é…symbolï¼ˆä¸é™æ—¶é—´ï¼Œå†å²æ–°é—»ï¼‰
                {'symbol': clean_code},
                {'symbol': stock_code},
            ]
            
            # ğŸ”¥ èåˆæŸ¥è¯¢ï¼šéå†æ‰€æœ‰æ¡ä»¶ï¼Œæ”¶é›†æ‰€æœ‰ç»“æœå¹¶å»é‡
            for query in specific_queries:
                if len(news_items) >= max_news:
                    break  # å·²ç»å¤Ÿäº†ï¼Œåœæ­¢æŸ¥è¯¢
                    
                try:
                    remaining = max_news - len(news_items)
                    cursor = collection.find(query).sort('publish_time', -1).limit(remaining * 2)  # å¤šå–ä¸€äº›ç”¨äºå»é‡
                    
                    for news in cursor:
                        title = news.get('title', '')
                        if title and title not in seen_titles:
                            seen_titles.add(title)
                            news_items.append(news)
                            if len(news_items) >= max_news:
                                break
                except Exception as e:
                    logger.debug(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æŸ¥è¯¢æ¡ä»¶å¤±è´¥: {e}")
                    continue
            
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š èåˆæŸ¥è¯¢è·å¾— {len(news_items)} æ¡æ–°é—»ï¼ˆå·²å»é‡ï¼‰")
            
            # ç¬¬äºŒæ­¥ï¼šå®æ—¶è·å–AKShareæ–°é—»ï¼ˆå¦‚æœä¸“å±æ–°é—»ä¸è¶³max_newsæ¡ï¼‰
            if len(news_items) < max_news:
                try:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ ä¸“å±æ–°é—»ä¸è¶³ï¼Œå°è¯•å®æ—¶è·å–AKShareæ–°é—»...")
                    
                    # åŠ¨æ€å¯¼å…¥AKShareé€‚é…å™¨
                    import asyncio
                    from app.worker.news_adapters.akshare_adapter import AKShareAdapter
                    
                    # åˆ›å»ºé€‚é…å™¨å¹¶è·å–å®æ—¶æ–°é—»
                    akshare = AKShareAdapter()
                    
                    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆå§‹åŒ–
                    loop = None
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # åˆå§‹åŒ–å¹¶è·å–æ–°é—»
                    if not loop.is_running():
                        loop.run_until_complete(akshare.initialize())
                        realtime_limit = min(10, max_news - len(news_items))  # æœ€å¤šè·å–10æ¡å®æ—¶æ–°é—»
                        realtime_news = loop.run_until_complete(akshare.get_news(stock_code, limit=realtime_limit))
                    else:
                        # å¦‚æœäº‹ä»¶å¾ªç¯å·²åœ¨è¿è¡Œï¼Œä½¿ç”¨åŒæ­¥æ–¹å¼
                        import nest_asyncio
                        nest_asyncio.apply()
                        loop.run_until_complete(akshare.initialize())
                        realtime_limit = min(10, max_news - len(news_items))
                        realtime_news = loop.run_until_complete(akshare.get_news(stock_code, limit=realtime_limit))
                    
                    if realtime_news:
                        # å»é‡åæ·»åŠ 
                        added_count = 0
                        for news in realtime_news:
                            title = news.get('title', '')
                            if title and title not in seen_titles:
                                seen_titles.add(title)
                                news_items.append(news)
                                added_count += 1
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”¥ å®æ—¶è·å– {added_count} æ¡AKShareæ–°é—»ï¼ˆå»é‡åï¼‰")
                    else:
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ å®æ—¶AKShareæœªè¿”å›æ–°é—»")
                        
                except Exception as e:
                    logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ å®æ—¶è·å–AKShareæ–°é—»å¤±è´¥: {e}")
            
            # ç¬¬ä¸‰æ­¥ï¼šè¡¥å……RSSé€šç”¨æ–°é—»ï¼ˆå¦‚æœä»ç„¶ä¸è¶³max_newsæ¡ï¼‰
            if len(news_items) < max_news:
                rss_limit = max_news - len(news_items)  # è®¡ç®—è¿˜éœ€è¦å¤šå°‘æ¡
                rss_query = {'symbol': 'GENERAL', 'source': 'RSS', 'publish_time': {'$gte': thirty_days_ago}}
                rss_cursor = collection.find(rss_query).sort('publish_time', -1).limit(rss_limit * 2)
                
                added_count = 0
                for news in rss_cursor:
                    title = news.get('title', '')
                    if title and title not in seen_titles:
                        seen_titles.add(title)
                        news_items.append(news)
                        added_count += 1
                        if len(news_items) >= max_news:
                            break
                
                if added_count > 0:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° è¡¥å…… {added_count} æ¡RSSé€šç”¨æ–°é—»")
            
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š æœ€ç»ˆè·å¾— {len(news_items)} æ¡æ–°é—»")

            if not news_items:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ° {stock_code} æˆ– {company_name} çš„ç›¸å…³æ–°é—»")
                return ""

            # æ ¼å¼åŒ–æ–°é—»
            report = f"# {stock_code} æœ€æ–°æ–°é—» (æ•°æ®åº“ç¼“å­˜)\n\n"
            report += f"ğŸ“… æŸ¥è¯¢æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            report += f"ğŸ“Š æ–°é—»æ•°é‡: {len(news_items)} æ¡\n\n"

            for i, news in enumerate(news_items, 1):
                title = news.get('title', 'æ— æ ‡é¢˜')
                content = news.get('content', '') or news.get('summary', '')
                source = news.get('source', 'æœªçŸ¥æ¥æº')
                publish_time = news.get('publish_time', datetime.now())
                sentiment = news.get('sentiment', 'neutral')

                # æƒ…ç»ªå›¾æ ‡
                sentiment_icon = {
                    'positive': 'ğŸ“ˆ',
                    'negative': 'ğŸ“‰',
                    'neutral': 'â–'
                }.get(sentiment, 'â–')

                report += f"## {i}. {sentiment_icon} {title}\n\n"
                report += f"**æ¥æº**: {source} | **æ—¶é—´**: {publish_time.strftime('%Y-%m-%d %H:%M') if isinstance(publish_time, datetime) else publish_time}\n"
                report += f"**æƒ…ç»ª**: {sentiment}\n\n"

                if content:
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    content_preview = content[:500] + '...' if len(content) > 500 else content
                    report += f"{content_preview}\n\n"

                report += "---\n\n"

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æˆåŠŸä»æ•°æ®åº“è·å–å¹¶æ ¼å¼åŒ– {len(news_items)} æ¡æ–°é—»")
            return report

        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ä»æ•°æ®åº“è·å–æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return ""

    def _sync_news_from_akshare(self, stock_code: str, max_news: int = 10) -> bool:
        """
        ä»AKShareåŒæ­¥æ–°é—»åˆ°æ•°æ®åº“ï¼ˆåŒæ­¥æ–¹æ³•ï¼‰
        ä½¿ç”¨åŒæ­¥çš„æ•°æ®åº“å®¢æˆ·ç«¯å’Œæ–°çº¿ç¨‹ä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª

        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_news: æœ€å¤§æ–°é—»æ•°é‡

        Returns:
            bool: æ˜¯å¦åŒæ­¥æˆåŠŸ
        """
        try:
            import asyncio
            import concurrent.futures

            # æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç ï¼ˆå»é™¤åç¼€ï¼‰
            clean_code = stock_code.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                   .replace('.XSHE', '').replace('.XSHG', '').replace('.HK', '')

            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”„ å¼€å§‹åŒæ­¥ {clean_code} çš„æ–°é—»...")

            # ğŸ”¥ åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼Œä½¿ç”¨åŒæ­¥æ•°æ®åº“å®¢æˆ·ç«¯
            def run_sync_in_new_thread():
                """åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯å¹¶è¿è¡ŒåŒæ­¥ä»»åŠ¡"""
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)

                try:
                    # å®šä¹‰å¼‚æ­¥è·å–æ–°é—»ä»»åŠ¡
                    async def get_news_task():
                        try:
                            # åŠ¨æ€å¯¼å…¥ AKShare providerï¼ˆæ­£ç¡®çš„å¯¼å…¥è·¯å¾„ï¼‰
                            from tradingagents.dataflows.providers.china.akshare import AKShareProvider

                            # åˆ›å»º provider å®ä¾‹
                            provider = AKShareProvider()

                            # è°ƒç”¨ provider è·å–æ–°é—»
                            news_data = await provider.get_stock_news(
                                symbol=clean_code,
                                limit=max_news
                            )

                            return news_data

                        except Exception as e:
                            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ è·å–æ–°é—»å¤±è´¥: {e}")
                            import traceback
                            logger.error(traceback.format_exc())
                            return None

                    # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è·å–æ–°é—»
                    news_data = new_loop.run_until_complete(get_news_task())

                    if not news_data:
                        logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æœªè·å–åˆ°æ–°é—»æ•°æ®")
                        return False

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¥ è·å–åˆ° {len(news_data)} æ¡æ–°é—»")

                    # ğŸ”¥ ä½¿ç”¨åŒæ­¥æ–¹æ³•ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¸ä¾èµ–äº‹ä»¶å¾ªç¯ï¼‰
                    from app.services.news_data_service import NewsDataService

                    news_service = NewsDataService()
                    saved_count = news_service.save_news_data_sync(
                        news_data=news_data,
                        data_source="akshare",
                        market="CN"
                    )

                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… åŒæ­¥æˆåŠŸ: {saved_count} æ¡æ–°é—»")
                    return saved_count > 0

                finally:
                    # æ¸…ç†äº‹ä»¶å¾ªç¯
                    new_loop.close()

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥ä»»åŠ¡ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª")
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_sync_in_new_thread)
                result = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                return result

        except concurrent.futures.TimeoutError:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
            return False
        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ åŒæ­¥æ–°é—»å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    async def _fetch_from_scraper(self, stock_code: str, keyword: str = "") -> str:
        """å¼‚æ­¥ä»Playwrightçˆ¬è™«è·å–æ–°é—» (é€šç”¨æ–¹æ³•)"""
        if not ScraperAdapter:
            return ""
            
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ å¯åŠ¨ScraperAdapterçˆ¬å– {stock_code} {keyword} ...")
            adapter = ScraperAdapter()
            # ä¼˜å…ˆä½¿ç”¨å…³é”®è¯ï¼Œå¦åˆ™ä½¿ç”¨ä»£ç 
            query = keyword if keyword else f"{stock_code} è‚¡ç¥¨ æœ€æ–°æ–°é—»"
            
            # è°ƒç”¨çˆ¬è™«
            results = await adapter.search(query)
            
            if results:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ çˆ¬è™«è¿”å› {len(results)} æ¡ç»“æœ")
                formatted = ""
                for item in results[:5]: # å–å‰5æ¡
                     title = item.get('title', 'æ— æ ‡é¢˜')
                     link = item.get('url', '#')
                     snippet = item.get('content', '')[:100]
                     formatted += f"### {title}\n- **æ¥æº**: Playwrightçˆ¬è™«\n- **é“¾æ¥**: {link}\n- **æ‘˜è¦**: {snippet}...\n\n"
                return formatted
            return ""
        except Exception as e:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ çˆ¬è™«æ‰§è¡Œå‡ºé”™: {e}")
            return ""

    def _get_a_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """è·å–Aè‚¡æ–°é—» - ğŸ”¥ å¹¶è¡Œèåˆç­–ç•¥"""
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸš€ è·å–Aè‚¡ {stock_code} æ–°é—» (å¹¶è¡Œèåˆæ¨¡å¼)")
        
        curr_date = datetime.now().strftime("%Y-%m-%d")
        company_name = self._get_company_name_from_code(stock_code)
        
        all_content_parts = []
        sources_used = []
        
        # ==================== æ•°æ®æº1: ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—» ====================
        try:
            if hasattr(self.toolkit, 'get_realtime_stock_news'):
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° [1/5] å°è¯•ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»...")
                result = self.toolkit.get_realtime_stock_news.invoke({"ticker": stock_code, "curr_date": curr_date})
                
                if result and len(result.strip()) > 100:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… ä¸œæ–¹è´¢å¯Œæ–°é—»: {len(result)} å­—ç¬¦")
                    all_content_parts.append(("ä¸œæ–¹è´¢å¯Œå®æ—¶æ–°é—»", result))
                    sources_used.append("ä¸œæ–¹è´¢å¯Œ")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}")

        # ==================== æ•°æ®æº2: AKShareå¤šæºè´¢ç»å¿«è®¯ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ [2/5] ä»AKShareèšåˆå¤šæºå¿«è®¯...")
            from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
            provider = get_akshare_provider()
            multi_news = provider.get_multi_source_news(limit_per_source=5)
            if multi_news and len(multi_news) > 200:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… AKShareå¤šæºå¿«è®¯: {len(multi_news)} å­—ç¬¦")
                all_content_parts.append(("AKShareå¤šæºå¿«è®¯", multi_news))
                sources_used.append("å¤šæºå¿«è®¯")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ AKShareå¤šæºå¿«è®¯è·å–å¤±è´¥: {e}")

        # ==================== æ•°æ®æº3: Google/Serperæ–°é—» ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” [3/5] å°è¯•Google/Serperæ–°é—»...")
            query = f"{stock_code} è‚¡ç¥¨ æ–°é—» è´¢æŠ¥"
            
            # ä¼˜å…ˆ Serper
            serper_result = self._search_news_with_serper(query, period="qdr:w")
            if serper_result and len(serper_result) > 50:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Serperæ–°é—»: {len(serper_result)} å­—ç¬¦")
                all_content_parts.append(("Google/Serperæ–°é—»", serper_result))
                sources_used.append("Google/Serper")
            elif hasattr(self.toolkit, 'get_google_news'):
                # å›é€€ Google çˆ¬è™«
                result = self.toolkit.get_google_news.invoke({"query": query, "curr_date": curr_date})
                if result and len(result.strip()) > 50:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleçˆ¬è™«: {len(result)} å­—ç¬¦")
                    all_content_parts.append(("Googleæ–°é—»(çˆ¬è™«)", result))
                    sources_used.append("Googleçˆ¬è™«")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] Google/Serperè·å–å¤±è´¥: {e}")

        # ==================== æ•°æ®æº4: Playwrightçˆ¬è™« ====================
        try:
            if ScraperAdapter:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ [4/5] è°ƒç”¨ Playwright çˆ¬è™«è·å–æ·±åº¦æ–°é—»...")
                
                # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                def run_scraper_cn():
                    keyword = company_name if company_name else stock_code
                    return asyncio.run(self._fetch_from_scraper(stock_code, keyword))
                
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_scraper_cn)
                    scraper_news = future.result(timeout=60) # 60ç§’è¶…æ—¶
                
                if scraper_news:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Playwrightçˆ¬è™«è·å–æˆåŠŸ: {len(scraper_news)} å­—ç¬¦")
                    all_content_parts.append(("Playwrightçˆ¬è™«", scraper_news))
                    sources_used.append("Playwrightçˆ¬è™«")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Playwrightçˆ¬è™«è°ƒç”¨å¤±è´¥: {e}")

        # ==================== æ•°æ®æº5: yfinanceæ–°é—»ï¼ˆæ¸¯è‚¡/ç¾è‚¡ï¼‰ ====================
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ¸¯è‚¡æˆ–ç¾è‚¡
            is_hk_or_us = stock_code.endswith('.HK') or (len(stock_code) <= 5 and not stock_code.isdigit())
            
            if is_hk_or_us:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° [5/6] å°è¯•yfinanceæ–°é—»...")
                import yfinance as yf
                
                ticker = yf.Ticker(stock_code)
                news_list = ticker.news
                
                if news_list:
                    yf_news_parts = []
                    for i, n in enumerate(news_list[:10], 1):  # æœ€å¤š10æ¡
                        content = n.get('content', {})
                        title = content.get('title', '')
                        summary = content.get('summary', '')[:200] if content.get('summary') else ''
                        provider = content.get('provider', {}).get('displayName', 'æœªçŸ¥æ¥æº')
                        pub_date = content.get('pubDate', '')[:10]
                        
                        yf_news_parts.append(f"### {i}. {title}")
                        if summary:
                            yf_news_parts.append(f"{summary}...")
                        yf_news_parts.append(f"**æ¥æº**: {provider} | **æ—¶é—´**: {pub_date}\n")
                    
                    yf_news_str = "\n".join(yf_news_parts)
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… yfinanceæ–°é—»: {len(news_list)} æ¡")
                    all_content_parts.append(("yfinanceæ–°é—»", yf_news_str))
                    sources_used.append("yfinance")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ yfinanceæ–°é—»è·å–å¤±è´¥: {e}")

        # ==================== æ•°æ®æº6: æ•°æ®åº“ç¼“å­˜ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¦ [6/6] ä»æ•°æ®åº“è·å–å†å²æ–°é—»...")
            db_news = self._get_news_from_database(stock_code, 10, company_name)
            if db_news:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“ç¼“å­˜: {len(db_news)} å­—ç¬¦")
                all_content_parts.append(("æ•°æ®åº“ç¼“å­˜", db_news))
                sources_used.append("æ•°æ®åº“")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] æ•°æ®åº“æ–°é—»è·å–å¤±è´¥: {e}")

        # ==================== èåˆæ‰€æœ‰æ•°æ®æº ====================
        if not all_content_parts:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ Aè‚¡æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥ï¼")
            return "âŒ æ— æ³•è·å–Aè‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"

        # ğŸ”¥ æ™ºèƒ½å»é‡
        deduplicator = NewsDeduplicator()
        deduplicated_parts = []
        total_before = 0
        total_after = 0
        
        for source_name, content in all_content_parts:
            lines = content.split('\n')
            deduplicated_lines = []
            for line in lines:
                # ç®€å•å»é‡é€»è¾‘ï¼šæå–æ ‡é¢˜è¡Œè¿›è¡Œåˆ¤æ–­
                title_to_check = ""
                if line.strip().startswith('##') or line.strip().startswith('###'):
                    title_to_check = re.sub(r'^#+\s*\d*\.\s*[ğŸ“ˆğŸ“‰â–ğŸ“°ğŸ“ŒğŸ“»]*\s*', '', line.strip())
                elif '**' in line and line.strip().startswith('-'):
                    match = re.search(r'\*\*(.+?)\*\*', line)
                    if match:
                        title_to_check = match.group(1)
                
                if title_to_check:
                    total_before += 1
                    if deduplicator.check_and_add(title_to_check, threshold=0.75):
                        deduplicated_lines.append(line)
                        total_after += 1
                else:
                    deduplicated_lines.append(line)
            
            deduplicated_content = '\n'.join(deduplicated_lines)
            if deduplicated_content.strip():
                deduplicated_parts.append((source_name, deduplicated_content))
                
        # æ„å»ºèåˆæŠ¥å‘Š
        total_chars = sum(len(content) for _, content in deduplicated_parts)
        duplicates_removed = total_before - total_after
        
        report = f"# {stock_code} ç»¼åˆæ–°é—»æŠ¥å‘Š (Aè‚¡å¤šæºèåˆ)\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ•°æ®æ¥æº: {', '.join(sources_used)} ({len(sources_used)}ä¸ª)\n"
        report += f"ğŸ“ æ€»æ•°æ®é‡: {total_chars} å­—ç¬¦\n"
        if duplicates_removed > 0:
            report += f"ğŸ”„ å»é‡ç»Ÿè®¡: {duplicates_removed} æ¡é‡å¤å·²ç§»é™¤\n"
        report += "\n---\n\n"
        
        for source_name, content in deduplicated_parts:
            report += f"\n## ğŸ“Œ æ¥æº: {source_name}\n\n{content}\n\n---\n"
        
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ‰ Aè‚¡èåˆå®Œæˆ: {len(sources_used)}ä¸ªæ•°æ®æº, {len(report)} å­—ç¬¦")
        return self._format_news_result(report, f"Aè‚¡å¤šæºèåˆ({','.join(sources_used)})", model_info)
    
    def _get_hk_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """
        è·å–æ¸¯è‚¡æ–°é—» - ğŸ”¥ å¹¶è¡Œèåˆç­–ç•¥
        åŒæ—¶ä»å¤šä¸ªæ•°æ®æºè·å–æ–°é—»ï¼Œèåˆå»é‡åè¿”å›
        """
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸš€ è·å–æ¸¯è‚¡ {stock_code} æ–°é—» (å¹¶è¡Œèåˆæ¨¡å¼)")
        
        # è·å–å½“å‰æ—¥æœŸ
        curr_date = datetime.now().strftime("%Y-%m-%d")
        company_name = self._get_company_name_from_code(stock_code)
        
        # ğŸ”¥ å¹¶è¡Œæ”¶é›†æ‰€æœ‰æ•°æ®æºçš„æ–°é—»
        # ğŸ”¥ æ–°é¡ºåºï¼šå®æ—¶æ•°æ®ä¼˜å…ˆï¼Œæ•°æ®åº“ç¼“å­˜æ”¾æœ€åï¼ˆé¿å…æ—§æ•°æ®æ˜¾ç¤ºåœ¨å‰é¢ï¼‰
        all_news = []  # å­˜å‚¨æ ‡é¢˜å’Œæ¥æºçš„tupleç”¨äºå»é‡
        all_content_parts = []  # å­˜å‚¨æ ¼å¼åŒ–åçš„å†…å®¹ç‰‡æ®µ
        sources_used = []  # è®°å½•æˆåŠŸä½¿ç”¨çš„æ•°æ®æº
        
        # ==================== æ•°æ®æº1: ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»ï¼ˆæœ€æ–°ï¼‰====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° [1/6] ä»ä¸œæ–¹è´¢å¯Œè·å–æœ€æ–°ä¸ªè‚¡æ–°é—»...")
            from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
            clean_code = stock_code.replace('.HK', '').replace('.hk', '')
            provider = get_akshare_provider()
            news_df = provider.stock_news_em(symbol=clean_code)
            
            if news_df is not None and not news_df.empty:
                news_content = f"=== ğŸ“° ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»ï¼ˆæœ€æ–°ï¼‰===\n\n"
                news_count = min(10, len(news_df))
                
                for idx, row in news_df.head(news_count).iterrows():
                    title = row.get('æ ‡é¢˜', 'æ— æ ‡é¢˜')
                    time_val = row.get('å‘å¸ƒæ—¶é—´', '')
                    content = row.get('å†…å®¹', '')
                    content_preview = content[:200] + '...' if len(content) > 200 else content
                    news_content += f"### {title}\n- **æ—¶é—´**: {time_val}\n- **å†…å®¹**: {content_preview}\n\n"
                
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… ä¸œæ–¹è´¢å¯Œ: {news_count}æ¡, {len(news_content)} å­—ç¬¦")
                all_content_parts.append(("ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»", news_content))
                sources_used.append("ä¸œæ–¹è´¢å¯Œ")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ ä¸œæ–¹è´¢å¯Œè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº7: Playwrightçˆ¬è™« ====================
        try:
            if ScraperAdapter:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ [7/7] è°ƒç”¨ Playwright çˆ¬è™«è·å–æ·±åº¦æ–°é—»...")
                
                # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                def run_scraper():
                    # ä¼˜å…ˆä½¿ç”¨å…¬å¸åæœç´¢
                    keyword = company_name if company_name else stock_code
                    return asyncio.run(self._fetch_from_scraper(stock_code, keyword))
                
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_scraper)
                    scraper_news = future.result(timeout=60) # 60ç§’è¶…æ—¶
                
                if scraper_news:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Playwrightçˆ¬è™«è·å–æˆåŠŸ: {len(scraper_news)} å­—ç¬¦")
                    all_content_parts.append(("Playwrightçˆ¬è™«", scraper_news))
                    sources_used.append("Playwrightçˆ¬è™«")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Playwrightçˆ¬è™«è°ƒç”¨å¤±è´¥: {e}")

        # ==================== æ•°æ®æº2: AKShareå¤šæºè´¢ç»å¿«è®¯ï¼ˆå®æ—¶ï¼‰====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ [2/6] ä»AKShareèšåˆå¤šæºå¿«è®¯...")
            from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
            provider = get_akshare_provider()
            multi_news = provider.get_multi_source_news(limit_per_source=5)
            if multi_news and len(multi_news) > 200:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… AKShareå¤šæºå¿«è®¯: {len(multi_news)} å­—ç¬¦")
                all_content_parts.append(("AKShareå¤šæºå¿«è®¯", multi_news))
                sources_used.append("å¤šæºå¿«è®¯")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ AKShareå¤šæºå¿«è®¯è·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº3: Serperå®æ—¶æœç´¢ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” [3/6] ä»Serperè·å–å®æ—¶æ–°é—»...")
            query = f"{stock_code} {company_name or 'æ¸¯è‚¡'} æ–°é—» è´¢æŠ¥ åˆ†æ"
            serper_result = self._search_news_with_serper(query, period="qdr:d")  # è¿‡å»1å¤©
            if serper_result and len(serper_result) > 100:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Serper: {len(serper_result)} å­—ç¬¦")
                all_content_parts.append(("Serperå®æ—¶æœç´¢", serper_result))
                sources_used.append("Serper")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Serperè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº4: Alpha Vantageä¸ªè‚¡æ–°é—»ï¼ˆä»…ç¾è‚¡ï¼‰====================
        # Alpha Vantageåªæ”¯æŒç¾è‚¡æ ¼å¼ï¼Œæ¸¯è‚¡/Aè‚¡ä¼šæŠ¥é”™ï¼Œç›´æ¥è·³è¿‡èŠ‚çœæ—¶é—´
        is_hk_stock = '.HK' in stock_code.upper() or '.hk' in stock_code
        is_a_stock = stock_code.isdigit() and len(stock_code) == 6
        
        if is_hk_stock or is_a_stock:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] â­ï¸ [4/6] Alpha Vantageè·³è¿‡ï¼ˆä»…æ”¯æŒç¾è‚¡ï¼Œå½“å‰: {'æ¸¯è‚¡' if is_hk_stock else 'Aè‚¡'}ï¼‰")
        else:
            try:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“ˆ [4/6] ä»Alpha Vantageè·å–ç¾è‚¡æ–°é—»...")
                from tradingagents.tools.alpha_vantage_news import get_alpha_vantage_news, format_alpha_vantage_news
                av_news = get_alpha_vantage_news(ticker=stock_code, limit=10)
                if av_news and len(av_news) > 0:
                    formatted = format_alpha_vantage_news(av_news, stock_code)
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Alpha Vantage: {len(av_news)}æ¡, {len(formatted)} å­—ç¬¦")
                    all_content_parts.append(("Alpha Vantage", formatted))
                    sources_used.append("Alpha Vantage")
            except Exception as e:
                logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Alpha Vantageè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº5: yfinanceæ–°é—»ï¼ˆæ¸¯è‚¡ä¸“ç”¨ï¼‰ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° [5/7] å°è¯•yfinanceæ–°é—»...")
            import yfinance as yf
            
            # ğŸ”§ ä¿®å¤ï¼šæ¸¯è‚¡ä»£ç å»é™¤å‰å¯¼0ï¼ˆ01810.HK â†’ 1810.HKï¼‰
            yf_code = stock_code
            if '.HK' in stock_code.upper():
                code_part = stock_code.upper().replace('.HK', '')
                code_part = code_part.lstrip('0') or '0'  # å»é™¤å‰å¯¼0ï¼Œä½†ä¿ç•™å•ä¸ª0
                yf_code = f"{code_part}.HK"
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ æ¸¯è‚¡ä»£ç è½¬æ¢: {stock_code} â†’ {yf_code}")
            
            ticker = yf.Ticker(yf_code)
            news_list = ticker.news
            
            # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•è¿”å›å€¼è¯¦æƒ…
            logger.info(f"[yfinanceè°ƒè¯•] è¿”å›ç±»å‹: {type(news_list)}")
            logger.info(f"[yfinanceè°ƒè¯•] è¿”å›å€¼: {news_list}")
            logger.info(f"[yfinanceè°ƒè¯•] æ˜¯å¦ä¸ºç©º: {not news_list}")
            logger.info(f"[yfinanceè°ƒè¯•] é•¿åº¦: {len(news_list) if news_list else 0}")
            
            if news_list:
                logger.info(f"[yfinanceè°ƒè¯•] âœ“ æ¡ä»¶é€šè¿‡ï¼Œå¼€å§‹å¤„ç†æ•°æ®...")
                yf_news_content = "=== ğŸ“° yfinanceæ–°é—» ===\n\n"
                for i, n in enumerate(news_list[:10], 1):
                    content = n.get('content', {})
                    title = content.get('title', '')
                    summary = content.get('summary', '')[:200] if content.get('summary') else ''
                    provider = content.get('provider', {}).get('displayName', 'æœªçŸ¥æ¥æº')
                    pub_date = content.get('pubDate', '')[:10]
                    
                    yf_news_content += f"### {i}. {title}\n"
                    if summary:
                        yf_news_content += f"{summary}...\n"
                    yf_news_content += f"**æ¥æº**: {provider} | **æ—¶é—´**: {pub_date}\n\n"
                
                logger.info(f"[yfinanceè°ƒè¯•] å‡†å¤‡æ·»åŠ åˆ°all_content_partsï¼Œå½“å‰é•¿åº¦: {len(all_content_parts)}")
                all_content_parts.append(("yfinanceæ–°é—»", yf_news_content))
                logger.info(f"[yfinanceè°ƒè¯•] å·²æ·»åŠ ï¼Œæ–°é•¿åº¦: {len(all_content_parts)}")
                sources_used.append("yfinance")
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… yfinanceæ–°é—»: {len(news_list)} æ¡")
            else:
                logger.warning(f"[yfinanceè°ƒè¯•] âœ— æ¡ä»¶æœªé€šè¿‡ï¼Œnews_listä¸ºç©ºæˆ–False")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ yfinanceæ–°é—»è·å–å¤±è´¥: {e}")
            logger.exception(f"[yfinanceè°ƒè¯•] å®Œæ•´å¼‚å¸¸ä¿¡æ¯:")

        # ==================== æ•°æ®æº6: RSSæ–°é—»æº ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¡ [6/7] ä»RSSæºè·å–æ–°é—»...")
            from app.worker.news_adapters.rss_adapter import RSSAdapter
            import asyncio
            
            rss_adapter = RSSAdapter()
            
            # è·å–äº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            # åˆå§‹åŒ–å¹¶è·å–æ–°é—»
            if not loop.is_running():
                rss_news = loop.run_until_complete(rss_adapter.get_news(symbol=stock_code, limit=15))
            else:
                import nest_asyncio
                nest_asyncio.apply()
                rss_news = loop.run_until_complete(rss_adapter.get_news(symbol=stock_code, limit=15))
            
            if rss_news and len(rss_news) > 0:
                rss_content = f"=== ğŸ“» RSSè´¢ç»å¿«è®¯ ===\n\n"
                rss_content += f"ğŸ“Š æ¥æº: é‡‘åæ•°æ®ã€è´¢è”ç¤¾ã€æ ¼éš†æ±‡ã€åå°”è¡—è§é—»ã€Google News\n\n"
                
                for news in rss_news[:15]:
                    title = news.get('title', '')
                    source = news.get('source', 'RSS')
                    match_type = news.get('match_type', '')
                    rss_content += f"- **{title}** [{source}] ({match_type})\n"
                
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… RSSæ–°é—»: {len(rss_news)}æ¡, {len(rss_content)} å­—ç¬¦")
                all_content_parts.append(("RSSæ–°é—»", rss_content))
                sources_used.append("RSS")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ RSSæ–°é—»è·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº7: æ•°æ®åº“ç¼“å­˜ï¼ˆè¡¥å……å†å²æ–°é—»ï¼‰====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¦ [7/7] ä»æ•°æ®åº“è·å–å†å²æ–°é—»...")
            db_news = self._get_news_from_database(stock_code, 15, company_name)  # å–15æ¡
            if db_news and len(db_news) > 100:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“ç¼“å­˜: {len(db_news)} å­—ç¬¦")
                all_content_parts.append(("æ•°æ®åº“å†å²", db_news))
                sources_used.append("æ•°æ®åº“å†å²")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æ•°æ®åº“è·å–å¤±è´¥: {e}")
        
        # ==================== èåˆæ‰€æœ‰æ•°æ®æº ====================
        if not all_content_parts:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥ï¼")
            return "âŒ æ— æ³•è·å–æ¸¯è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
        
        # ğŸ”¥ æ™ºèƒ½å»é‡ï¼šè·¨æ•°æ®æºå»é‡
        deduplicator = NewsDeduplicator()
        deduplicated_parts = []
        total_before = 0
        total_after = 0
        
        for source_name, content in all_content_parts:
            # æå–å†…å®¹ä¸­çš„æ‰€æœ‰æ ‡é¢˜è¡Œ
            lines = content.split('\n')
            deduplicated_lines = []
            
            for line in lines:
                # æ£€æµ‹æ ‡é¢˜è¡Œï¼ˆä»¥##ã€###å¼€å¤´æˆ–åŒ…å«**ç²—ä½“**ï¼‰
                is_title_line = False
                title_to_check = ""
                
                if line.strip().startswith('##') or line.strip().startswith('###'):
                    is_title_line = True
                    # æå–æ ‡é¢˜å†…å®¹
                    title_to_check = re.sub(r'^#+\s*\d*\.\s*[ğŸ“ˆğŸ“‰â–ğŸ“°ğŸ“ŒğŸ“»]*\s*', '', line.strip())
                elif '**' in line and line.strip().startswith('-'):
                    is_title_line = True
                    # æå–**å†…å®¹**
                    match = re.search(r'\*\*(.+?)\*\*', line)
                    if match:
                        title_to_check = match.group(1)
                
                if is_title_line and title_to_check:
                    total_before += 1
                    # ä½¿ç”¨æ™ºèƒ½å»é‡åˆ¤æ–­
                    if deduplicator.check_and_add(title_to_check, threshold=0.75):
                        deduplicated_lines.append(line)
                        total_after += 1
                    else:
                        # é‡å¤çš„æ ‡é¢˜ï¼Œè·³è¿‡è¿™ä¸€è¡Œ
                        logger.debug(f"[å»é‡] è·³è¿‡é‡å¤æ ‡é¢˜: {title_to_check[:30]}...")
                else:
                    # éæ ‡é¢˜è¡Œï¼Œç›´æ¥ä¿ç•™
                    deduplicated_lines.append(line)
            
            # é‡å»ºå†…å®¹
            deduplicated_content = '\n'.join(deduplicated_lines)
            if deduplicated_content.strip():
                deduplicated_parts.append((source_name, deduplicated_content))
        
        # è®¡ç®—å»é‡ç»Ÿè®¡
        duplicates_removed = total_before - total_after
        if duplicates_removed > 0:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”„ æ™ºèƒ½å»é‡: {duplicates_removed} æ¡é‡å¤æ–°é—»å·²ç§»é™¤ (åŸ{total_before}æ¡ â†’ {total_after}æ¡)")
        
        # æ„å»ºèåˆæŠ¥å‘Š
        total_chars = sum(len(content) for _, content in deduplicated_parts)
        report = f"# {stock_code} ç»¼åˆæ–°é—»æŠ¥å‘Š (å¤šæºèåˆ+æ™ºèƒ½å»é‡)\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ•°æ®æ¥æº: {', '.join(sources_used)} ({len(sources_used)}ä¸ª)\n"
        report += f"ğŸ“ æ€»æ•°æ®é‡: {total_chars} å­—ç¬¦\n"
        if duplicates_removed > 0:
            report += f"ğŸ”„ å»é‡ç»Ÿè®¡: {duplicates_removed} æ¡é‡å¤å·²ç§»é™¤\n"
        report += "\n---\n\n"
        
        # æŒ‰æ•°æ®æºåˆ†ç»„è¾“å‡º
        for source_name, content in deduplicated_parts:
            report += f"\n## ğŸ“Œ æ¥æº: {source_name}\n\n"
            report += content
            report += "\n---\n"
        
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ‰ èåˆå®Œæˆ: {len(sources_used)}ä¸ªæ•°æ®æº, {len(report)} å­—ç¬¦, å»é‡{duplicates_removed}æ¡")
        
        return self._format_news_result(report, f"å¤šæºèåˆ({','.join(sources_used)})", model_info)
    
    def get_stock_sentiment_unified(
        self,
        ticker: str,
        curr_date: str
    ) -> dict:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        å¯¹äºAè‚¡å’Œæ¸¯è‚¡ï¼Œä½¿ç”¨Serper APIæŠ“å–é›ªçƒå’Œè‚¡å§çš„çœŸå®æ•£æˆ·è¯„è®º

        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼ˆå¦‚ï¼š000001ã€0700.HKã€AAPLï¼‰
            curr_date: å½“å‰æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰

        Returns:
            dict: åŒ…å«æƒ…ç»ªåˆ†ææŠ¥å‘Šå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] åˆ†æè‚¡ç¥¨: {ticker}")

        try:
            from tradingagents.utils.stock_utils import StockUtils

            # è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹
            market_info = StockUtils.get_market_info(ticker)
            is_china = market_info['is_china']
            is_hk = market_info['is_hk']
            is_us = market_info['is_us']

            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] è‚¡ç¥¨ç±»å‹: {market_info['market_name']}")

            result_data = []
            
            # åˆå§‹åŒ–é»˜è®¤è¿”å›å­—å…¸
            response_dict = {
                "ticker": ticker,
                "stock_type": market_info['market_name'],
                "date": curr_date,
                "sentiment": "Neutral",
                "score": 0.5,
                "summary": "åˆ†æä¸­...",
                "confidence": "low",
                "content": ""
            }

            if is_china or is_hk:
                # ä¸­å›½Aè‚¡å’Œæ¸¯è‚¡ï¼šä½¿ç”¨Serper APIæœç´¢é›ªçƒå’Œè‚¡å§
                logger.info(f"ğŸ‡¨ğŸ‡³ğŸ‡­ğŸ‡° [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] ä½¿ç”¨Serperæœç´¢ä¸­æ–‡å¸‚åœºæƒ…ç»ª...")
                
                try:
                    import requests
                    import os
                    import re
                    
                    serper_api_key = os.getenv("SERPER_API_KEY")
                    if not serper_api_key:
                        raise ValueError("æœªé…ç½®SERPER_API_KEY")
                        
                    # å¤„ç†è‚¡ç¥¨åç§°
                    clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '').replace('.HK', '')
                    stock_name = market_info.get('name', '').replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                    
                    # ã€å®‰å…¨é”ã€‘ä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œé»˜è®¤ä¸º Tickerï¼Œé˜²æ­¢æŸ¥åº“å¤±è´¥å¯¼è‡´å˜é‡ç¼ºå¤±
                    real_name = clean_ticker
                    
                    # å°è¯•ä»æ•°æ®åº“è·å–çœŸå®å…¬å¸å
                    try:
                        # ã€å…³é”®ã€‘å±€éƒ¨å¼•ç”¨ï¼Œé˜²æ­¢ Circular Import å¯¼è‡´å…¨ç«™å´©æºƒ
                        from tradingagents.dataflows.interface import get_stock_name_by_ticker
                        
                        # å°è¯•æŸ¥è¯¢
                        name_from_db = get_stock_name_by_ticker(clean_ticker)
                        if name_from_db:
                            real_name = name_from_db
                            logger.info(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âœ… ä»æ•°æ®åº“è·å–åˆ°å…¬å¸å: {real_name}")
                        else:
                            logger.info(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âš ï¸ æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ° {clean_ticker} çš„å…¬å¸åï¼Œä½¿ç”¨ä»£ç : {real_name}")
                    except Exception as e:
                        # ã€å…³é”®ã€‘é™é»˜å¤±è´¥ï¼šå¦‚æœ‰ä»»ä½•æŠ¥é”™ï¼Œç›´æ¥å¿½ç•¥ï¼Œä»…æ‰“å°è­¦å‘Šï¼Œä¿è¯ç¨‹åºç»§ç»­è¿è¡Œ
                        logger.warning(f"[ç»Ÿä¸€æƒ…ç»ªå·¥å…·] âš ï¸ ä»æ•°æ®åº“è·å–å…¬å¸åå¤±è´¥ï¼Œä½¿ç”¨è‚¡ç¥¨ä»£ç : {clean_ticker}ï¼Œé”™è¯¯: {e}")
                        real_name = clean_ticker
                    
                    # ä½¿ç”¨ real_name æ›¿ä»£åŸæ¥çš„ stock_name å˜é‡
                    stock_name = real_name
                    
                    if not stock_name or stock_name == ticker:
                        # å°è¯•è·å–ä¸­æ–‡åç§°
                        try:
                            if is_hk:
                                from tradingagents.dataflows.interface import get_hk_stock_info_unified
                                info = get_hk_stock_info_unified(ticker)
                            else:
                                from tradingagents.dataflows.interface import get_china_stock_info_unified
                                info = get_china_stock_info_unified(ticker)
                            
                            if isinstance(info, dict) and 'name' in info:
                                stock_name = info['name']
                        except:
                            pass
                    
                    # å†æ¬¡æ¸…ç†åç§° (é˜²æ­¢å¸¦æœ‰"æ¸¯è‚¡"å‰ç¼€)
                    if stock_name:
                        stock_name = stock_name.replace('æ¸¯è‚¡', '').replace('Aè‚¡', '')
                        # å»é™¤å¸¸è§çš„åç¼€å’Œå†—ä½™è¯ (å¦‚ï¼šäº¬ä¸œé›†å›¢-SW -> äº¬ä¸œ)
                        # 1. å…ˆè¿›è¡ŒNFKCæ ‡å‡†åŒ–ï¼Œå°†å…¨è§’å­—ç¬¦è½¬ä¸ºåŠè§’
                        import unicodedata
                        stock_name = unicodedata.normalize('NFKC', stock_name)
                        # 2. å»é™¤é›†å›¢ã€è‚¡ä»½ç­‰åç¼€ï¼Œä»¥åŠ -SW, -W ç­‰åç¼€
                        stock_name = re.sub(r'(é›†å›¢|è‚¡ä»½|æœ‰é™å…¬å¸|ï¼.*|-.*|\(.*\)|ï¼ˆ.*ï¼‰)', '', stock_name)
                        stock_name = stock_name.strip()
                    
                    # æ„é€ ä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢ (å…¨ç½‘æœç´¢ï¼Œæ›´å®½æ¾çš„å…³é”®è¯)
                    # ç­–ç•¥ï¼šä½¿ç”¨å…¬å¸å+æƒ…ç»ªç›¸å…³å…³é”®è¯ï¼Œä¸é™å®šç‰¹å®šç½‘ç«™
                    search_queries = []
                    
                    # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šå…¬å¸å + æŠ•èµ„è®¨è®ºå…³é”®è¯ (å…¨ç½‘)
                    if stock_name and stock_name != ticker:
                        search_queries.append(f'{stock_name} æŠ•èµ„ åˆ†æ è®¨è®º')
                        search_queries.append(f'{stock_name} è‚¡ç¥¨ è§‚ç‚¹ è¯„è®º')
                    
                    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šè‚¡ç¥¨ä»£ç  + å…³é”®è¯
                    search_queries.append(f'{clean_ticker} è‚¡ç¥¨ åˆ†æ è§‚ç‚¹')
                    
                    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå…¬å¸å + é›ªçƒ/è‚¡å§ (ä½œä¸ºfallback)
                    if stock_name and stock_name != ticker:
                        search_queries.append(f'{stock_name} site:xueqiu.com OR site:guba.eastmoney.com')
                    
                    logger.info(f"ğŸ” [Serper] å‡†å¤‡æ‰§è¡Œ {len(search_queries)} ä¸ªæœç´¢ç­–ç•¥")
                    
                    url = "https://google.serper.dev/search"
                    headers = {
                        'X-API-KEY': serper_api_key,
                        'Content-Type': 'application/json'
                    }
                    
                    def perform_search(query, time_range="qdr:w"):
                        """æ‰§è¡Œæœç´¢ï¼Œé»˜è®¤è¿‡å»ä¸€å‘¨"""
                        payload = json.dumps({
                            "q": query,
                            "tbs": time_range,
                            "num": 15
                        })
                        response = requests.request("POST", url, headers=headers, data=payload, timeout=10)
                        return response.json().get('organic', [])
                    
                    organic_results = []
                    successful_query = None
                    
                    # ä¾æ¬¡å°è¯•å„ä¸ªæŸ¥è¯¢ç­–ç•¥
                    for query in search_queries:
                        logger.info(f"ğŸ” [Serper] å°è¯•æŸ¥è¯¢: {query}")
                        results = perform_search(query)
                        if results:
                            organic_results = results
                            successful_query = query
                            logger.info(f"âœ… [Serper] æŸ¥è¯¢æˆåŠŸï¼Œè·å– {len(results)} æ¡ç»“æœ")
                            break
                        else:
                            logger.warning(f"âš ï¸ [Serper] æŸ¥è¯¢æ— ç»“æœ: {query}")
                    
                    # å¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾çš„æŸ¥è¯¢ (è¿‡å»ä¸€ä¸ªæœˆ)
                    if not organic_results and stock_name:
                        logger.warning(f"âš ï¸ [Serper] æ‰€æœ‰ç­–ç•¥å¤±è´¥ï¼Œå°è¯•æ›´å®½æ¾æŸ¥è¯¢ (è¿‡å»ä¸€ä¸ªæœˆ)")
                        fallback_query = f'{stock_name} è‚¡ç¥¨'
                        organic_results = perform_search(fallback_query, "qdr:m")
                        if organic_results:
                            successful_query = fallback_query
                            logger.info(f"âœ… [Serper] å®½æ¾æŸ¥è¯¢æˆåŠŸï¼Œè·å– {len(organic_results)} æ¡ç»“æœ")
                    
                    if organic_results:
                        discussions = []
                        for item in organic_results:
                            title = item.get('title', '')
                            snippet = item.get('snippet', '')
                            source = item.get('link', '')
                            
                            # æ™ºèƒ½è¯†åˆ«æ¥æºå¹³å°
                            platform = "ç½‘ç»œ"
                            if "xueqiu.com" in source:
                                platform = "é›ªçƒ"
                            elif "eastmoney.com" in source or "guba" in source:
                                platform = "è‚¡å§"
                            elif "sina.com" in source:
                                platform = "æ–°æµª"
                            elif "163.com" in source:
                                platform = "ç½‘æ˜“"
                            elif "qq.com" in source:
                                platform = "è…¾è®¯"
                            elif "baidu.com" in source or "baijiahao" in source:
                                platform = "ç™¾åº¦"
                            elif "zhihu.com" in source:
                                platform = "çŸ¥ä¹"
                            elif "toutiao.com" in source:
                                platform = "å¤´æ¡"
                            elif "weibo.com" in source:
                                platform = "å¾®åš"
                            elif "36kr.com" in source:
                                platform = "36æ°ª"
                            elif "wallstreetcn.com" in source:
                                platform = "åå°”è¡—è§é—»"
                            elif "cls.cn" in source:
                                platform = "è´¢è”ç¤¾"
                                
                            discussions.append(f"- [{platform}] **{title}**: {snippet}")
                        
                        discussion_text = "\n".join(discussions)
                        
                        sentiment_summary = f"""
## å¸‚åœºæƒ…ç»ªä¸æŠ•èµ„è€…è§‚ç‚¹åˆ†æ (Serperå…¨ç½‘æœç´¢)

**è‚¡ç¥¨**: {ticker} ({stock_name})
**åˆ†ææ—¥æœŸ**: {curr_date}
**æœç´¢ç­–ç•¥**: {successful_query}
**ç»“æœæ•°é‡**: {len(organic_results)} æ¡

### æŠ•èµ„è€…è®¨è®ºä¸è§‚ç‚¹
{discussion_text}

### æƒ…ç»ªåˆ†æè¦æ±‚
è¯·åŸºäºä¸Šè¿°æœç´¢ç»“æœï¼Œåˆ†æï¼š
1. æŠ•èµ„è€…æ•´ä½“æƒ…ç»ªå€¾å‘ï¼ˆä¹è§‚/æ‚²è§‚/ä¸­æ€§ï¼‰
2. ä¸»è¦å…³æ³¨ç‚¹å’Œè®¨è®ºè¯é¢˜
3. æ½œåœ¨çš„é£é™©ç‚¹æˆ–åˆ©å¥½å› ç´ 
"""
                        result_data.append(sentiment_summary)
                        response_dict["content"] = sentiment_summary
                        response_dict["status"] = "success"
                        response_dict["source"] = "Serper/Googleå…¨ç½‘æœç´¢"
                        logger.info(f"âœ… [Serper] æˆåŠŸè·å– {len(organic_results)} æ¡æŠ•èµ„è€…è®¨è®º")
                    else:
                        # ğŸ”¥ é™çº§æ–¹æ¡ˆï¼šSerper æ— ç»“æœæ—¶ï¼Œè°ƒç”¨æ–°é—»å·¥å…·è¡¥ä½
                        logger.warning(f"âš ï¸ [Serper] æœä¸åˆ°ç¤¾äº¤åª’ä½“æ•°æ®ï¼Œå°è¯•è°ƒç”¨æ–°é—»å·¥å…·è¡¥ä½...")
                        
                        # è°ƒç”¨ get_stock_news_unified è·å–æœ€æ–°æ–°é—»
                        news_res = self.get_stock_news_unified(ticker, max_news=20)
                        news_content = news_res.get("content", "")
                        
                        if news_content and "æ— æ³•è·å–" not in news_content:
                            sentiment_summary = f"""
## å¸‚åœºæƒ…ç»ªä¸èˆ†æƒ…åˆ†æ (ç”±äºç¤¾äº¤åª’ä½“æ•°æ®å—é™ï¼Œå·²åˆ‡æ¢è‡³æ·±åº¦æ–°é—»èˆ†æƒ…æ¨¡å¼)

**è‚¡ç¥¨**: {ticker} ({stock_name})
**åˆ†ææ—¥æœŸ**: {curr_date}
**è¡¥ä½æº**: æ ¸å¿ƒå…¬å‘Šä¸ä¸»æµè´¢ç»æ–°é—»

### æ ¸å¿ƒèˆ†æƒ…å†…å®¹
{news_content}

### æƒ…ç»ªåˆ¤ç ”æ‰§è¡ŒæŒ‡ä»¤
ç”±äºå½“å‰ç¤¾äº¤åª’ä½“ï¼ˆé›ªçƒ/è‚¡å§ï¼‰å®æ—¶è®¨è®ºæŠ“å–å—é™ï¼Œè¯·ä½ ä½œä¸ºåˆ†æå¸ˆï¼š
1. **ä»¥å…¬å‘Šä¸ºå‡†**ï¼šé‡ç‚¹ç ”è¯»å…¬å‘Šï¼ˆNoticeï¼‰ä¸­çš„å›è´­ã€è‚¡æƒå˜åŠ¨ç­‰å®é”¤ä¿¡æ¯ã€‚
2. **èˆ†æƒ…æ¨æµ‹**ï¼šé€šè¿‡ä¸»æµåª’ä½“ï¼ˆè¯åˆ¸æ—¶æŠ¥ã€è´¢è”ç¤¾ç­‰ï¼‰çš„æŠ¥é“åŸºè°ƒï¼Œæ¨æ–­æœºæ„å’Œæ•£æˆ·çš„å¸‚åœºé¢„æœŸã€‚
3. **æƒ…ç»ªæ˜ å°„**ï¼šåŸºäºä¸šåŠ¡è¿›å±•ï¼ˆcmsArticleWebOldï¼‰å’Œèµ„é‡‘æµå‘ï¼Œåˆ»ç”»å½“å‰å¸‚åœºå¯¹è¯¥æ ‡çš„çš„å†·çƒ­ç¨‹åº¦ã€‚
"""
                            result_data.append(sentiment_summary)
                            response_dict["content"] = sentiment_summary
                            response_dict["status"] = "warning"
                            response_dict["source"] = "æ–°é—»èˆ†æƒ…è¡¥ä½"
                            logger.info(f"âœ… [æƒ…ç»ªè¡¥ä½] å·²é€šè¿‡æ–°é—»æºæˆåŠŸè¡¥ä½ï¼Œé•¿åº¦: {len(news_content)}")
                        else:
                            logger.warning(f"âš ï¸ [Serper] ä¸”æ–°é—»æºäº¦æ— æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼")
                            response_dict["summary"] = f"å½“å‰æš‚æ— è‚¡ç¥¨ {ticker} ({stock_name}) çš„è¯¦ç»†å¸‚åœºæƒ…ç»ªæ•°æ®ï¼Œç³»ç»Ÿé»˜è®¤ç»™äºˆä¸­æ€§è¯„çº§ã€‚è¯·ä»¥å¸‚åœºå®¢è§‚æŒ‡æ ‡ä¸ºå‡†ã€‚"
                            response_dict["company_name_check"] = f"è¯·ä¸¥æ ¼åŸºäºè‚¡ç¥¨ä»£ç  {ticker} ç¡®è®¤å…¬å¸åç§°ï¼Œç¦æ­¢è‡†æµ‹"
                            return response_dict


                except Exception as e:
                    # ğŸ”¥ æ•è· Serper å¼‚å¸¸ï¼ˆå¦‚ Key ç¼ºå¤±ï¼‰ï¼ŒåŒæ ·å°è¯•æ–°é—»è¡¥ä½
                    logger.error(f"âŒ [Serper] æœç´¢å¤±è´¥: {e}ï¼Œå°è¯•æ‰§è¡Œæ–°é—»é™çº§æ–¹æ¡ˆ...")
                    
                    try:
                        news_res = self.get_stock_news_unified(ticker, max_news=20)
                        news_content = news_res.get("content", "")
                        
                        if news_content and "æ— æ³•è·å–" not in news_content:
                             sentiment_summary = f"""
## èˆ†æƒ…çƒ­åº¦åˆ†ææŠ¥å‘Š (å¤‡ç”¨æ¨¡å¼)

**åˆ†æå¯¹è±¡**: {ticker}
**åˆ†ææ—¥æœŸ**: {curr_date}
**æ•°æ®æ¥æº**: å®æ—¶è´¢ç»æ–°é—»ä¸å®˜æ–¹å…¬å‘Š

{news_content}

---
**åˆ†æå¸ˆæç¤º**ï¼šå½“å‰ç¤¾äº¤å¹³å°æ•°æ®æ¥å£ç»´æŠ¤ä¸­ï¼Œè¯·åŸºäºä¸Šè¿°æƒå¨æ–°é—»å’Œå…¬å‘Šå†…å®¹ï¼Œç ”åˆ¤æŠ•èµ„è€…çš„å¿ƒç†é¢„æœŸå’Œå¸‚åœºåšå¼ˆå½¢æ€ã€‚
"""
                             result_data.append(sentiment_summary)
                             response_dict["content"] = sentiment_summary
                             response_dict["status"] = "warning"
                             response_dict["source"] = "æ–°é—»èˆ†æƒ…é™çº§"
                             return response_dict
                    except Exception as nested_e:
                        logger.error(f"âŒ [é™çº§å¤±è´¥] æ–°é—»è¡¥ä½ä¹ŸæŒ‚äº†: {nested_e}")

                    response_dict["summary"] = f"ã€ç³»ç»Ÿæç¤ºã€‘æƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}"
                    response_dict["status"] = "error"
                    response_dict["error"] = str(e)
                    return response_dict


            else:
                # ç¾è‚¡ï¼šä½¿ç”¨Redditæƒ…ç»ªåˆ†æ
                logger.info(f"ğŸ‡ºğŸ‡¸ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] å¤„ç†ç¾è‚¡æƒ…ç»ª...")

                try:
                    from tradingagents.dataflows.interface import get_reddit_sentiment

                    sentiment_data = get_reddit_sentiment(ticker, curr_date)
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\n{sentiment_data}")
                    response_dict["content"] = sentiment_data
                    response_dict["source"] = "Reddit"
                    response_dict["status"] = "success"
                except Exception as e:
                    result_data.append(f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}")
                    response_dict["content"] = f"## ç¾è‚¡Redditæƒ…ç»ª\nè·å–å¤±è´¥: {e}"
                    response_dict["status"] = "error"

            # ç»„åˆæ‰€æœ‰æ•°æ®
            combined_result = f"""# {ticker} æƒ…ç»ªåˆ†æ

**è‚¡ç¥¨ç±»å‹**: {market_info['market_name']}
**åˆ†ææ—¥æœŸ**: {curr_date}

{chr(10).join(result_data)}

---
*æ•°æ®æ¥æº: Serper (Google Search) / Reddit*
"""
            response_dict["content"] = combined_result
            
            logger.info(f"ğŸ˜Š [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] æ•°æ®è·å–å®Œæˆï¼Œæ€»é•¿åº¦: {len(combined_result)}")
            return response_dict

        except Exception as e:
            error_msg = f"ç»Ÿä¸€æƒ…ç»ªåˆ†æå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ [ç»Ÿä¸€æƒ…ç»ªå·¥å…·] {error_msg}")
            return {
                "status": "error",
                "error": str(e),
                "ticker": ticker,
                "content": error_msg
            }

    def _get_us_share_news(self, stock_code: str, max_news: int, model_info: str = "") -> str:
        """
        è·å–ç¾è‚¡æ–°é—» - å¤šæºèåˆæ¨¡å¼
        æ•°æ®æºï¼šFinnHub + Alpha Vantage + Serper + æ™ºèƒ½å»é‡
        """
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ‡ºğŸ‡¸ è·å–ç¾è‚¡ {stock_code} æ–°é—»ï¼ˆå¤šæºèåˆæ¨¡å¼ï¼‰")
        
        all_content_parts = []  # æ”¶é›†æ‰€æœ‰æ•°æ®æºçš„å†…å®¹
        sources_used = []
        
        # ==================== æ•°æ®æº1: FinnHubæ–°é—»ï¼ˆæœ€ä¸°å¯Œï¼‰====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“° [1/4] ä»FinnHubè·å–æ–°é—»...")
            import requests
            import os
            from datetime import datetime, timedelta
            
            api_key = os.getenv('FINNHUB_API_KEY', '')
            if api_key:
                end_date = datetime.now().strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
                url = f'https://finnhub.io/api/v1/company-news?symbol={stock_code}&from={start_date}&to={end_date}&token={api_key}'
                
                resp = requests.get(url, timeout=15)
                if resp.status_code == 200:
                    news_list = resp.json()
                    if news_list and len(news_list) > 0:
                        finnhub_content = f"=== ğŸ“° FinnHubç¾è‚¡æ–°é—» ===\n\n"
                        for n in news_list[:15]:  # å–å‰15æ¡
                            headline = n.get('headline', '')
                            source = n.get('source', '')
                            summary = n.get('summary', '')[:200] + '...' if len(n.get('summary', '')) > 200 else n.get('summary', '')
                            pub_time = datetime.fromtimestamp(n.get('datetime', 0)).strftime('%Y-%m-%d %H:%M') if n.get('datetime') else ''
                            finnhub_content += f"### {headline}\n- **æ—¶é—´**: {pub_time}\n- **æ¥æº**: {source}\n- **æ‘˜è¦**: {summary}\n\n"
                        
                        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… FinnHub: {min(15, len(news_list))}æ¡, {len(finnhub_content)} å­—ç¬¦")
                        all_content_parts.append(("FinnHub", finnhub_content))
                        sources_used.append("FinnHub")
            else:
                logger.warning("[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ FINNHUB_API_KEYæœªé…ç½®")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ FinnHubè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº2: Alpha Vantageæ–°é—» ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“ˆ [2/4] ä»Alpha Vantageè·å–æ–°é—»...")
            from tradingagents.tools.alpha_vantage_news import get_alpha_vantage_news, format_alpha_vantage_news
            av_news = get_alpha_vantage_news(ticker=stock_code, limit=10)
            if av_news and len(av_news) > 0:
                formatted = format_alpha_vantage_news(av_news, stock_code)
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Alpha Vantage: {len(av_news)}æ¡, {len(formatted)} å­—ç¬¦")
                all_content_parts.append(("Alpha Vantage", formatted))
                sources_used.append("Alpha Vantage")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Alpha Vantageè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº3: Serperå®æ—¶æœç´¢ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ” [3/4] ä»Serperè·å–å®æ—¶æ–°é—»...")
            query = f"{stock_code} stock news earnings financial analysis"
            serper_result = self._search_news_with_serper(query, period="qdr:d")  # è¿‡å»1å¤©
            if serper_result and len(serper_result) > 100:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Serper: {len(serper_result)} å­—ç¬¦")
                all_content_parts.append(("Serperå®æ—¶æœç´¢", serper_result))
                sources_used.append("Serper")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Serperè·å–å¤±è´¥: {e}")
        
        # ==================== æ•°æ®æº4: æ•°æ®åº“ç¼“å­˜ ====================
        try:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“¦ [4/5] ä»æ•°æ®åº“è·å–å†å²æ–°é—»...")
            db_news = self._get_news_from_database(stock_code, 10, stock_code)
            if db_news and len(db_news) > 100:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… æ•°æ®åº“ç¼“å­˜: {len(db_news)} å­—ç¬¦")
                all_content_parts.append(("æ•°æ®åº“å†å²", db_news))
                sources_used.append("æ•°æ®åº“å†å²")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ æ•°æ®åº“è·å–å¤±è´¥: {e}")

        # ==================== æ•°æ®æº5: Playwrightçˆ¬è™« ====================
        try:
            if ScraperAdapter:
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ•·ï¸ [5/5] è°ƒç”¨ Playwright çˆ¬è™«è·å–æ·±åº¦æ–°é—»...")
                
                # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                def run_scraper_us():
                    return asyncio.run(self._fetch_from_scraper(stock_code, stock_code))
                
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(run_scraper_us)
                    scraper_news = future.result(timeout=60) # 60ç§’è¶…æ—¶
                
                if scraper_news:
                    logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Playwrightçˆ¬è™«è·å–æˆåŠŸ: {len(scraper_news)} å­—ç¬¦")
                    all_content_parts.append(("Playwrightçˆ¬è™«", scraper_news))
                    sources_used.append("Playwrightçˆ¬è™«")
        except Exception as e:
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Playwrightçˆ¬è™«è°ƒç”¨å¤±è´¥: {e}")
        
        # ==================== èåˆæ‰€æœ‰æ•°æ®æº ====================
        if not all_content_parts:
            logger.error(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âŒ æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥ï¼")
            return "âŒ æ— æ³•è·å–ç¾è‚¡æ–°é—»æ•°æ®ï¼Œæ‰€æœ‰æ–°é—»æºå‡ä¸å¯ç”¨"
        
        # ğŸ”¥ æ™ºèƒ½å»é‡
        deduplicator = NewsDeduplicator()
        deduplicated_parts = []
        total_before = 0
        
        for source_name, content in all_content_parts:
            lines = content.split('\n')
            kept_lines = []
            for line in lines:
                if line.startswith('###') or line.startswith('- **'):
                    total_before += 1
                    title = line.replace('###', '').replace('- **', '').strip()
                    if deduplicator.check_and_add(title):
                        kept_lines.append(line)
                else:
                    kept_lines.append(line)
            deduplicated_parts.append((source_name, '\n'.join(kept_lines)))
        
        stats = deduplicator.get_stats()
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”„ æ™ºèƒ½å»é‡: {stats['duplicates_removed']} æ¡é‡å¤å·²ç§»é™¤ (åŸ{stats['total_checked']}æ¡ â†’ {stats['unique_kept']}æ¡)")
        
        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        final_report = f"""# {stock_code} ç»¼åˆæ–°é—»æŠ¥å‘Š (å¤šæºèåˆ+æ™ºèƒ½å»é‡)

ğŸ“… ç”Ÿæˆæ—¶é—´: {timestamp}
ğŸ“Š æ•°æ®æ¥æº: {', '.join(sources_used)} ({len(sources_used)}ä¸ª)
ğŸ“ æ€»æ•°æ®é‡: {sum(len(c) for _, c in deduplicated_parts)} å­—ç¬¦
ğŸ”„ å»é‡ç»Ÿè®¡: {stats['duplicates_removed']} æ¡é‡å¤å·²ç§»é™¤

---

"""
        for source_name, content in deduplicated_parts:
            final_report += f"## ğŸ“Œ æ¥æº: {source_name}\n\n{content}\n\n---\n\n"
        
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ‰ ç¾è‚¡èåˆå®Œæˆ: {len(sources_used)}ä¸ªæ•°æ®æº, {len(final_report)} å­—ç¬¦")
        
        return final_report
    
    def _format_news_result(self, news_content: str, source: str, model_info: str = "") -> str:
        """æ ¼å¼åŒ–æ–°é—»ç»“æœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ğŸ” æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°åŸå§‹æ–°é—»å†…å®¹
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“‹ åŸå§‹æ–°é—»å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦): {news_content[:500]}")
        logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ“Š åŸå§‹å†…å®¹é•¿åº¦: {len(news_content)} å­—ç¬¦")
        
        # æ£€æµ‹æ˜¯å¦ä¸ºGoogle/Geminiæ¨¡å‹
        is_google_model = any(keyword in model_info.lower() for keyword in ['google', 'gemini', 'gemma'])
        original_length = len(news_content)
        google_control_applied = False
        
        # ğŸ” æ·»åŠ Googleæ¨¡å‹æ£€æµ‹æ—¥å¿—
        if is_google_model:
            logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ¤– æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†")
        
        # å¯¹Googleæ¨¡å‹è¿›è¡Œç‰¹æ®Šçš„é•¿åº¦æ§åˆ¶
        if is_google_model and len(news_content) > 5000:  # é™ä½é˜ˆå€¼åˆ°5000å­—ç¬¦
            logger.warning(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ æ£€æµ‹åˆ°Googleæ¨¡å‹ï¼Œæ–°é—»å†…å®¹è¿‡é•¿({len(news_content)}å­—ç¬¦)ï¼Œè¿›è¡Œé•¿åº¦æ§åˆ¶...")
            
            # æ›´ä¸¥æ ¼çš„é•¿åº¦æ§åˆ¶ç­–ç•¥
            lines = news_content.split('\n')
            important_lines = []
            char_count = 0
            target_length = 3000  # ç›®æ ‡é•¿åº¦è®¾ä¸º3000å­—ç¬¦
            
            # ç¬¬ä¸€è½®ï¼šä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®è¯çš„é‡è¦è¡Œ
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
                important_keywords = ['è‚¡ç¥¨', 'å…¬å¸', 'è´¢æŠ¥', 'ä¸šç»©', 'æ¶¨è·Œ', 'ä»·æ ¼', 'å¸‚å€¼', 'è¥æ”¶', 'åˆ©æ¶¦', 
                                    'å¢é•¿', 'ä¸‹è·Œ', 'ä¸Šæ¶¨', 'ç›ˆåˆ©', 'äºæŸ', 'æŠ•èµ„', 'åˆ†æ', 'é¢„æœŸ', 'å…¬å‘Š']
                
                is_important = any(keyword in line for keyword in important_keywords)
                
                if is_important and char_count + len(line) < target_length:
                    important_lines.append(line)
                    char_count += len(line)
                elif not is_important and char_count + len(line) < target_length * 0.7:  # éé‡è¦å†…å®¹æ›´ä¸¥æ ¼é™åˆ¶
                    important_lines.append(line)
                    char_count += len(line)
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡é•¿åº¦ï¼Œåœæ­¢æ·»åŠ 
                if char_count >= target_length:
                    break
            
            # å¦‚æœæå–çš„é‡è¦å†…å®¹ä»ç„¶è¿‡é•¿ï¼Œè¿›è¡Œè¿›ä¸€æ­¥æˆªæ–­
            if important_lines:
                processed_content = '\n'.join(important_lines)
                if len(processed_content) > target_length:
                    processed_content = processed_content[:target_length] + "...(å†…å®¹å·²æ™ºèƒ½æˆªæ–­)"
                
                news_content = processed_content
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âœ… Googleæ¨¡å‹æ™ºèƒ½é•¿åº¦æ§åˆ¶å®Œæˆï¼Œä»{original_length}å­—ç¬¦å‹ç¼©è‡³{len(news_content)}å­—ç¬¦")
            else:
                # å¦‚æœæ²¡æœ‰é‡è¦è¡Œï¼Œç›´æ¥æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                news_content = news_content[:target_length] + "...(å†…å®¹å·²å¼ºåˆ¶æˆªæ–­)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] âš ï¸ Googleæ¨¡å‹å¼ºåˆ¶æˆªæ–­è‡³{target_length}å­—ç¬¦")
        
        # è®¡ç®—æœ€ç»ˆçš„æ ¼å¼åŒ–ç»“æœé•¿åº¦ï¼Œç¡®ä¿æ€»é•¿åº¦åˆç†
        base_format_length = 300  # æ ¼å¼åŒ–æ¨¡æ¿çš„å¤§æ¦‚é•¿åº¦
        if is_google_model and (len(news_content) + base_format_length) > 4000:
            # å¦‚æœåŠ ä¸Šæ ¼å¼åŒ–åä»ç„¶è¿‡é•¿ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ–°é—»å†…å®¹
            max_content_length = 3500
            if len(news_content) > max_content_length:
                news_content = news_content[:max_content_length] + "...(å·²ä¼˜åŒ–é•¿åº¦)"
                google_control_applied = True
                logger.info(f"[ç»Ÿä¸€æ–°é—»å·¥å…·] ğŸ”§ Googleæ¨¡å‹æœ€ç»ˆé•¿åº¦ä¼˜åŒ–ï¼Œå†…å®¹é•¿åº¦: {len(news_content)}å­—ç¬¦")
        
        # æ„å»ºæ ¼å¼åŒ–ç»“æœ
        formatted_result = f"""
=== ğŸ“° æ–°é—»æ•°æ®æ¥æº: {source} ===
è·å–æ—¶é—´: {timestamp}
æ•°æ®é•¿åº¦: {len(news_content)} å­—ç¬¦
{f"æ¨¡å‹ç±»å‹: {model_info}" if model_info else ""}
{f"ğŸ”§ Googleæ¨¡å‹é•¿åº¦æ§åˆ¶å·²åº”ç”¨ (åŸé•¿åº¦: {original_length} å­—ç¬¦)" if google_control_applied else ""}

=== ğŸ“‹ æ–°é—»å†…å®¹ ===
{news_content}

=== âœ… æ•°æ®çŠ¶æ€ ===
çŠ¶æ€: æˆåŠŸè·å–
æ¥æº: {source}
æ—¶é—´æˆ³: {timestamp}
"""
        return formatted_result.strip()

    def _get_market_news_comprehensive(self, max_news_per_source: int = 3) -> str:
        """
        è·å–å¸‚åœºæ•´ä½“æ–°é—»ï¼ˆç»¼åˆå¤šä¸ªæ¥æºï¼‰
        è°ƒç”¨åˆ†ç±».mdä¸­çš„æ–°é—»æ¥å£ï¼Œæä¾›å¸‚åœºæ•´ä½“è§†è§’çš„æ–°é—»
        
        Args:
            max_news_per_source: æ¯ä¸ªæ•°æ®æºæœ€å¤šè·å–çš„æ–°é—»æ•°é‡
            
        Returns:
            str: æ ¼å¼åŒ–çš„å¸‚åœºæ–°é—»å†…å®¹
        """
        from tradingagents.dataflows.providers.china.akshare import get_akshare_provider
        
        logger.info(f"[å¸‚åœºæ–°é—»] å¼€å§‹è·å–å¸‚åœºæ•´ä½“æ–°é—»ï¼Œæ¯æºé™åˆ¶{max_news_per_source}æ¡")
        
        provider = get_akshare_provider()
        news_sections = []
        
        # 1. è´¢æ–°ç½‘ - è´¢ç»å†…å®¹ç²¾é€‰
        try:
            logger.info("[å¸‚åœºæ–°é—»] å°è¯•è·å–è´¢æ–°ç½‘æ–°é—»...")
            df_cx = provider.stock_news_main_cx()
            if not df_cx.empty:
                formatted = self._format_caixin_news(df_cx, max_news_per_source)
                if formatted:
                    news_sections.append(formatted)
                    logger.info(f"[å¸‚åœºæ–°é—»] âœ… è´¢æ–°ç½‘æ–°é—»è·å–æˆåŠŸ: {len(df_cx)}æ¡")
        except Exception as e:
            logger.warning(f"[å¸‚åœºæ–°é—»] âš ï¸ è´¢æ–°ç½‘æ–°é—»è·å–å¤±è´¥: {e}")
        
        # 2. ä¸œè´¢ - è´¢ç»æ—©é¤
        try:
            logger.info("[å¸‚åœºæ–°é—»] å°è¯•è·å–è´¢ç»æ—©é¤...")
            df_breakfast = provider.stock_info_cjzc_em()
            if not df_breakfast.empty:
                formatted = self._format_breakfast_news(df_breakfast, max_news_per_source)
                if formatted:
                    news_sections.append(formatted)
                    logger.info(f"[å¸‚åœºæ–°é—»] âœ… è´¢ç»æ—©é¤è·å–æˆåŠŸ: {len(df_breakfast)}æ¡")
        except Exception as e:
            logger.warning(f"[å¸‚åœºæ–°é—»] âš ï¸ è´¢ç»æ—©é¤è·å–å¤±è´¥: {e}")
        
        # 3-7. å…¨çƒè´¢ç»å¿«è®¯ï¼ˆå®Œæ•´çš„5ä¸ªæ¥æºï¼‰
        sources = [
            ('ä¸œæ–¹è´¢å¯Œ', provider.stock_info_global_em),
            ('æ–°æµªè´¢ç»', provider.stock_info_global_sina),
            ('åŒèŠ±é¡º', provider.stock_info_global_ths),
            ('è´¢è”ç¤¾', provider.stock_info_global_cls),
            ('å¯Œé€”ç‰›ç‰›', provider.stock_info_global_futu)
        ]
        
        for source_name, source_func in sources:
            try:
                logger.info(f"[å¸‚åœºæ–°é—»] å°è¯•è·å–{source_name}å¿«è®¯...")
                df = source_func()
                if not df.empty:
                    formatted = self._format_global_news(df, source_name, max_news_per_source)
                    if formatted:
                        news_sections.append(formatted)
                        logger.info(f"[å¸‚åœºæ–°é—»] âœ… {source_name}å¿«è®¯è·å–æˆåŠŸ: {len(df)}æ¡")
            except Exception as e:
                logger.warning(f"[å¸‚åœºæ–°é—»] âš ï¸ {source_name}å¿«è®¯è·å–å¤±è´¥: {e}")
        
        # ç»„åˆæ‰€æœ‰æ•°æ®
        if news_sections:
            result = "\n\n=== ğŸ“Š å¸‚åœºæ•´ä½“æ–°é—»è¡¥å…… ===\n\n" + "\n\n".join(news_sections)
            logger.info(f"[å¸‚åœºæ–°é—»] âœ… å¸‚åœºæ–°é—»æ±‡æ€»å®Œæˆï¼Œå…±{len(news_sections)}ä¸ªæ•°æ®æº")
            return result
        else:
            logger.warning("[å¸‚åœºæ–°é—»] âš ï¸ æœªè·å–åˆ°ä»»ä½•å¸‚åœºæ–°é—»æ•°æ®")
            return ""

    def _format_caixin_news(self, df, max_news: int) -> str:
        """æ ¼å¼åŒ–è´¢æ–°ç½‘æ–°é—»"""
        try:
            import pandas as pd
            if df is None or df.empty:
                return ""
            
            result = "=== ğŸ“° è´¢æ–°ç½‘è´¢ç»å†…å®¹ç²¾é€‰ ===\n"
            
            for idx, row in df.head(max_news).iterrows():
                title = row.get('æ ‡é¢˜', '') or row.get('title', '')
                time = row.get('å‘å¸ƒæ—¶é—´', '') or row.get('æ—¶é—´', '') or row.get('time', '')
                link = row.get('é“¾æ¥', '') or row.get('link', '')
                
                if title:
                    result += f"\n### {title}\n"
                    if time:
                        result += f"- **æ—¶é—´**: {time}\n"
                    if link:
                        result += f"- **é“¾æ¥**: {link}\n"
            
            return result if len(result) > 50 else ""
        except Exception as e:
            logger.warning(f"[å¸‚åœºæ–°é—»] æ ¼å¼åŒ–è´¢æ–°ç½‘æ–°é—»å¤±è´¥: {e}")
            return ""

    def _format_breakfast_news(self, df, max_news: int) -> str:
        """æ ¼å¼åŒ–è´¢ç»æ—©é¤æ•°æ®"""
        try:
            import pandas as pd
            if df is None or df.empty:
                return ""
            
            result = "=== ğŸŒ… ä¸œæ–¹è´¢å¯Œè´¢ç»æ—©é¤ ===\n"
            
            for idx, row in df.head(max_news).iterrows():
                title = row.get('æ ‡é¢˜', '') or row.get('title', '')
                date = row.get('æ—¥æœŸ', '') or row.get('date', '')
                link = row.get('é“¾æ¥', '') or row.get('link', '')
                
                if title:
                    result += f"\n### {title}\n"
                    if date:
                        result += f"- **æ—¥æœŸ**: {date}\n"
                    if link:
                        result += f"- **é“¾æ¥**: {link}\n"
            
            return result if len(result) > 50 else ""
        except Exception as e:
            logger.warning(f"[å¸‚åœºæ–°é—»] æ ¼å¼åŒ–è´¢ç»æ—©é¤å¤±è´¥: {e}")
            return ""

    def _format_global_news(self, df, source_name: str, max_news: int) -> str:
        """æ ¼å¼åŒ–å…¨çƒè´¢ç»å¿«è®¯æ•°æ®"""
        try:
            import pandas as pd
            if df is None or df.empty:
                return ""
            
            result = f"=== ğŸŒ {source_name}å…¨çƒè´¢ç»å¿«è®¯ ===\n"
            
            for idx, row in df.head(max_news).iterrows():
                # ä¸åŒæ¥æºçš„å­—æ®µåå¯èƒ½ä¸åŒï¼Œéœ€è¦é€‚é…
                title = row.get('æ ‡é¢˜', '') or row.get('title', '')
                content = row.get('å†…å®¹', '') or row.get('content', '') or row.get('æ‘˜è¦', '') or row.get('ç®€ä»‹', '')
                time = row.get('å‘å¸ƒæ—¶é—´', '') or row.get('æ—¶é—´', '') or row.get('time', '') or row.get('date', '')
                
                # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œä½¿ç”¨å†…å®¹å‰50å­—ç¬¦
                display_title = title if title else (content[:50] + "..." if content and len(content) > 50 else content)
                
                if display_title:
                    result += f"\n### {display_title}\n"
                    if time:
                        result += f"- **æ—¶é—´**: {time}\n"
                    if content and title:  # å¦‚æœæœ‰æ ‡é¢˜ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹
                        content_preview = content[:200] + "..." if len(content) > 200 else content
                        result += f"- **å†…å®¹**: {content_preview}\n"
            
            return result if len(result) > 50 else ""
        except Exception as e:
            logger.warning(f"[å¸‚åœºæ–°é—»] æ ¼å¼åŒ–{source_name}å¿«è®¯å¤±è´¥: {e}")
            return ""
        
        formatted_result = f"""
=== ğŸ“° æ–°é—»æ•°æ®æ¥æº: {source} ===
è·å–æ—¶é—´: {timestamp}
æ•°æ®é•¿åº¦: {len(news_content)} å­—ç¬¦
{f"æ¨¡å‹ç±»å‹: {model_info}" if model_info else ""}
{f"ğŸ”§ Googleæ¨¡å‹é•¿åº¦æ§åˆ¶å·²åº”ç”¨ (åŸé•¿åº¦: {original_length} å­—ç¬¦)" if google_control_applied else ""}

=== ğŸ“‹ æ–°é—»å†…å®¹ ===
{news_content}

=== âœ… æ•°æ®çŠ¶æ€ ===
çŠ¶æ€: æˆåŠŸè·å–
æ¥æº: {source}
æ—¶é—´æˆ³: {timestamp}
"""
        return formatted_result.strip()


def create_unified_news_tool(toolkit):
    """åˆ›å»ºç»Ÿä¸€æ–°é—»å·¥å…·å‡½æ•°"""
    analyzer = UnifiedNewsAnalyzer(toolkit)
    
    def get_stock_news_unified(stock_code: str, max_news: int = 100, model_info: str = ""):
        """
        ç»Ÿä¸€æ–°é—»è·å–å·¥å…·
        
        Args:
            stock_code (str): è‚¡ç¥¨ä»£ç  (æ”¯æŒAè‚¡å¦‚000001ã€æ¸¯è‚¡å¦‚0700.HKã€ç¾è‚¡å¦‚AAPL)
            max_news (int): æœ€å¤§æ–°é—»æ•°é‡ï¼Œé»˜è®¤100
            model_info (str): å½“å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºç‰¹æ®Šå¤„ç†
        """
        

        if not stock_code:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
        
        return analyzer.get_stock_news_unified(stock_code, max_news, model_info)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_news_unified.name = "get_stock_news_unified"
    get_stock_news_unified.description = """
ç»Ÿä¸€æ–°é—»è·å–å·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æ–°é—»

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ ¹æ®è‚¡ç¥¨ç±»å‹é€‰æ‹©æœ€ä½³æ–°é—»æº
- Aè‚¡: ä¼˜å…ˆä¸œæ–¹è´¢å¯Œ -> Googleä¸­æ–‡ -> OpenAI
- æ¸¯è‚¡: ä¼˜å…ˆGoogle -> OpenAI -> å®æ—¶æ–°é—»
- ç¾è‚¡: ä¼˜å…ˆOpenAI -> Googleè‹±æ–‡ -> FinnHub
- è¿”å›æ ¼å¼åŒ–çš„æ–°é—»å†…å®¹
- æ”¯æŒGoogleæ¨¡å‹çš„ç‰¹æ®Šé•¿åº¦æ§åˆ¶
"""
    
    def get_stock_sentiment_unified(ticker: str, curr_date: str) -> str:
        """
        ç»Ÿä¸€çš„è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…·
        è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡ï¼‰å¹¶è°ƒç”¨ç›¸åº”çš„æƒ…ç»ªæ•°æ®æº
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç 
            curr_date: å½“å‰æ—¥æœŸ
            
        """
        if not ticker:
            return "âŒ é”™è¯¯: æœªæä¾›è‚¡ç¥¨ä»£ç "
            
        return analyzer.get_stock_sentiment_unified(ticker, curr_date)
    
    # è®¾ç½®å·¥å…·å±æ€§
    get_stock_sentiment_unified.name = "get_stock_sentiment_unified"
    get_stock_sentiment_unified.description = """
ç»Ÿä¸€è‚¡ç¥¨æƒ…ç»ªåˆ†æå·¥å…· - æ ¹æ®è‚¡ç¥¨ä»£ç è‡ªåŠ¨è·å–ç›¸åº”å¸‚åœºçš„æƒ…ç»ªæ•°æ®

åŠŸèƒ½:
- è‡ªåŠ¨è¯†åˆ«è‚¡ç¥¨ç±»å‹ï¼ˆAè‚¡/æ¸¯è‚¡/ç¾è‚¡ï¼‰
- æ¸¯è‚¡/Aè‚¡: ä½¿ç”¨Serperæœç´¢é›ªçƒ/è‚¡å§çš„æ•£æˆ·è®¨è®º
- ç¾è‚¡: ä½¿ç”¨Reddit/Twitteræƒ…ç»ªæ•°æ®
- è¿”å›æ ¼å¼åŒ–çš„æƒ…ç»ªåˆ†ææŠ¥å‘Š
"""

    return get_stock_news_unified, get_stock_sentiment_unified