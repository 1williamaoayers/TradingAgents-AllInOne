"""
çˆ¬è™«æ–°é—»å®šæ—¶åŒæ­¥æœåŠ¡
åªåŒæ­¥è‡ªé€‰è‚¡ï¼Œæ™ºèƒ½è°ƒæ•´é‡‡é›†é¢‘ç‡ï¼Œæ”¯æŒå³æ—¶åŒå†™
"""
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


def get_sync_interval() -> int:
    """
    æ ¹æ®æ—¶é—´è¿”å›é‡‡é›†é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    - äº¤æ˜“æ—¶æ®µï¼ˆ9:30-16:00ï¼‰ï¼šæ¯1å°æ—¶
    - ç›˜å‰ç›˜åï¼ˆ6:00-20:00ï¼‰ï¼šæ¯2å°æ—¶
    - æ·±å¤œï¼ˆ20:00-6:00ï¼‰ï¼šæ¯6å°æ—¶
    - å‘¨æœ«/èŠ‚å‡æ—¥ï¼šæ¯12å°æ—¶
    """
    now = datetime.now()
    hour = now.hour
    minute = now.minute
    is_weekend = now.weekday() >= 5  # å‘¨å…­=5ï¼Œå‘¨æ—¥=6
    
    if is_weekend:
        return 720  # 12å°æ—¶
    
    # äº¤æ˜“æ—¶æ®µï¼ˆ9:30-16:00ï¼‰
    if (hour == 9 and minute >= 30) or (10 <= hour < 16):
        return 60   # 1å°æ—¶
    # ç›˜å‰ç›˜åï¼ˆ6:00-20:00ï¼‰
    elif 6 <= hour < 20:
        return 120  # 2å°æ—¶
    # æ·±å¤œ
    else:
        return 360  # 6å°æ—¶


class ScraperSyncService:
    """çˆ¬è™«æ–°é—»å®šæ—¶åŒæ­¥æœåŠ¡"""
    
    def __init__(self):
        self.scraper = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¾ªç¯å¯¼å…¥
        self.db = None
        self._initialized = False
        self.stats = {
            "total_syncs": 0,
            "last_sync_time": None,
            "last_sync_count": 0,
            "errors": []
        }
    
    async def initialize(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶çš„å¯¼å…¥é—®é¢˜"""
        if self._initialized:
            return
        
        try:
            # å¯¼å…¥ScraperAdapter
            from app.worker.news_adapters.scraper_adapter import ScraperAdapter
            self.scraper = ScraperAdapter()
            
            # è¿æ¥MongoDBï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„è®¤è¯ä¿¡æ¯ï¼‰
            from motor.motor_asyncio import AsyncIOMotorClient
            import os
            
            host = os.getenv("MONGODB_HOST", "mongodb")
            port = os.getenv("MONGODB_PORT", "27017")
            username = os.getenv("MONGODB_USERNAME", "")
            password = os.getenv("MONGODB_PASSWORD", "")
            database = os.getenv("MONGODB_DATABASE", "tradingagents")
            
            # æ„å»ºè¿æ¥URIï¼ˆæ”¯æŒæœ‰è®¤è¯å’Œæ— è®¤è¯ä¸¤ç§æƒ…å†µï¼‰
            if username and password:
                mongo_uri = f"mongodb://{username}:{password}@{host}:{port}/{database}?authSource=admin"
            else:
                mongo_uri = f"mongodb://{host}:{port}"
            
            mongo_client = AsyncIOMotorClient(mongo_uri)
            self.db = mongo_client[database]
            
            self._initialized = True
            logger.info("[ScraperSync] âœ… æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"[ScraperSync] âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def get_all_favorite_stocks(self) -> List[str]:
        """
        è·å–æ‰€æœ‰ç”¨æˆ·çš„è‡ªé€‰è‚¡åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        
        Returns:
            List[str]: å»é‡åçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        try:
            if self.db is None:
                await self.initialize()
            
            # æŸ¥è¯¢user_favoritesé›†åˆï¼ˆfavoritesæ˜¯åµŒå¥—æ•°ç»„ï¼‰
            collection = self.db.user_favorites
            cursor = collection.find({}, {'favorites': 1})
            
            favorites_set = set()
            async for doc in cursor:
                # favoritesæ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«stock_code
                favorites_list = doc.get('favorites', [])
                for fav in favorites_list:
                    stock_code = fav.get('stock_code') or fav.get('symbol')
                    if stock_code:
                        favorites_set.add(stock_code)
            
            logger.info(f"[ScraperSync] è·å–åˆ° {len(favorites_set)} åªè‡ªé€‰è‚¡")
            return list(favorites_set)
        
        except Exception as e:
            logger.error(f"[ScraperSync] è·å–è‡ªé€‰è‚¡å¤±è´¥: {e}")
            return []
    
    async def sync_favorite_stocks(self) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰è‡ªé€‰è‚¡æ–°é—»åˆ°æ•°æ®åº“
        
        Returns:
            Dict: åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        """
        start_time = datetime.now()
        
        try:
            await self.initialize()
            
            # è·å–è‡ªé€‰è‚¡åˆ—è¡¨
            favorites = await self.get_all_favorite_stocks()
            
            if not favorites:
                logger.info("[ScraperSync] æ²¡æœ‰è‡ªé€‰è‚¡ï¼Œè·³è¿‡åŒæ­¥")
                return {"status": "skipped", "reason": "no_favorites"}
            
            logger.info(f"[ScraperSync] ğŸš€ å¼€å§‹åŒæ­¥ {len(favorites)} åªè‡ªé€‰è‚¡...")
            
            # ç»Ÿè®¡
            success_count = 0
            total_news = 0
            total_fetched = 0  # æ–°å¢: ç»Ÿè®¡æŠ“å–æ€»æ•°
            errors = []
            
            # åˆ†æ‰¹å¹¶å‘é‡‡é›†ï¼ˆ5ä¸ªä¸€æ‰¹ï¼Œé¿å…è¿‡è½½ï¼‰
            batch_size = 5
            for i in range(0, len(favorites), batch_size):
                batch = favorites[i:i+batch_size]
                
                # å¹¶å‘é‡‡é›†è¿™ä¸€æ‰¹
                tasks = [self._sync_single_stock(stock) for stock in batch]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for stock, result in zip(batch, results):
                    if isinstance(result, Exception):
                        errors.append(f"{stock}: {str(result)}")
                    elif result and result.get("success"):
                        success_count += 1
                        total_news += result.get("saved_count", 0)
                        total_fetched += result.get("fetched_count", 0)  # ç´¯åŠ æŠ“å–æ•°
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯10ç§’ï¼Œé¿å…çˆ¬è™«æœåŠ¡è¿‡è½½
                if i + batch_size < len(favorites):
                    await asyncio.sleep(10)
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_syncs"] += 1
            self.stats["last_sync_time"] = datetime.now()
            self.stats["last_sync_count"] = total_news
            if errors:
                self.stats["errors"] = errors[-5:]  # åªä¿ç•™æœ€è¿‘5ä¸ªé”™è¯¯
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            result = {
                "status": "success",
                "favorites_count": len(favorites),
                "success_count": success_count,
                "total_news_saved": total_news,
                "total_fetched": total_fetched,
                "errors": errors,
                "elapsed_seconds": round(elapsed, 2)
            }
            
            logger.info(f"[ScraperSync] âœ… åŒæ­¥å®Œæˆ: {success_count}/{len(favorites)} æˆåŠŸ, "
                       f"æ–°å¢ {total_news} æ¡æ–°é—» (æŠ“å– {total_fetched}), è€—æ—¶ {elapsed:.1f}ç§’")

            # ä¿å­˜åŒæ­¥å†å² (æ–°å¢: ç¡®ä¿Scraperä»»åŠ¡å¯è§) 
            try:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                sync_record = {
                    "sync_type": "scraper", 
                    "total_stocks": len(favorites),
                    "success_count": success_count,
                    "news_count": total_news,
                    "fetched_count": total_fetched,  # æ–°å¢: è®°å½•æŠ“å–æ€»æ•°
                    "error_count": len(errors),
                    "duration": round(elapsed, 2),
                    "status": "success" if not errors else "partial",
                    "sync_time": datetime.utcnow(),  # ä½¿ç”¨ UTC æ—¶é—´ä»¥ä¿æŒä¸€è‡´æ€§
                    "created_at": datetime.utcnow()
                }
                
                # åªæœ‰å½“æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸæ—¶æ‰å†™å…¥
                if self.db is not None:
                     await self.db.news_sync_history.insert_one(sync_record)
                     logger.info(f"[ScraperSync] ğŸ“œ åŒæ­¥è®°å½•å·²å†™å…¥å†å²è¡¨: {success_count}æˆåŠŸ/{len(errors)}å¤±è´¥")
                else:
                    logger.warning("[ScraperSync] âš ï¸ æ•°æ®åº“æœªè¿æ¥ï¼Œæ— æ³•å†™å…¥åŒæ­¥å†å²")

            except Exception as e:
                logger.error(f"[ScraperSync] âŒ ä¿å­˜åŒæ­¥å†å²å¤±è´¥: {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"[ScraperSync] âŒ åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "error", "error": str(e)}
    
    async def _sync_single_stock(self, stock_code: str, max_retries: int = 3) -> Dict[str, Any]:
        """
        åŒæ­¥å•åªè‚¡ç¥¨æ–°é—»ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            Dict: åŒæ­¥ç»“æœ
        """
        for attempt in range(max_retries):
            try:
                # è°ƒç”¨çˆ¬è™«APIè·å–æ–°é—»
                news_list = await self.scraper.get_news(stock_code, limit=50)
                
                if not news_list:
                    logger.info(f"[ScraperSync] {stock_code}: æ— æ–°é—»æ•°æ®")
                    return {"success": True, "saved_count": 0}
                
                # å»é‡ä¿å­˜åˆ°æ•°æ®åº“
                saved_count = await self._save_with_dedup(news_list, stock_code)
                
                logger.info(f"[ScraperSync] {stock_code}: é‡‡é›† {len(news_list)} æ¡, æ–°å¢ {saved_count} æ¡")
                return {"success": True, "saved_count": saved_count, "fetched_count": len(news_list)}
                
            except Exception as e:
                if attempt < max_retries - 1:
                    logger.warning(f"[ScraperSync] {stock_code} é‡‡é›†å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
                    await asyncio.sleep(30)  # ç­‰å¾…30ç§’åé‡è¯•
                else:
                    logger.error(f"[ScraperSync] {stock_code} é‡‡é›†å¤±è´¥ (å·²é‡è¯• {max_retries} æ¬¡): {e}")
                    return {"success": False, "error": str(e)}
        
        return {"success": False, "error": "max_retries_exceeded"}
    
    async def _save_with_dedup(self, news_list: List[Dict], stock_code: str) -> int:
        """
        å»é‡ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            int: å®é™…ä¿å­˜çš„æ¡æ•°
        """
        if self.db is None:
            await self.initialize()
        
        collection = self.db.stock_news
        saved_count = 0
        
        for news in news_list:
            try:
                title = news.get('title', '')
                if not title:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ ‡é¢˜+æ¥æºå»é‡ï¼‰
                existing = await collection.find_one({
                    'title': title,
                    'source': news.get('source', 'scraper')
                })
                
                if not existing:
                    # æ·»åŠ å…ƒæ•°æ®
                    news_doc = {
                        **news,
                        'symbol': stock_code,
                        'source_type': 'scraper',
                        'sync_time': datetime.now(),
                        'created_at': datetime.now()
                    }
                    
                    await collection.insert_one(news_doc)
                    saved_count += 1
                    
            except Exception as e:
                logger.warning(f"[ScraperSync] ä¿å­˜æ–°é—»å¤±è´¥: {e}")
                continue
        
        return saved_count
    
    async def cache_news_immediately(self, news_list: List[Dict], stock_code: str):
        """
        å³æ—¶åŒå†™ï¼šå°†æ–°é—»ç¼“å­˜åˆ°æ•°æ®åº“ï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            stock_code: è‚¡ç¥¨ä»£ç 
        """
        try:
            if not self._initialized:
                await self.initialize()
            
            saved_count = await self._save_with_dedup(news_list, stock_code)
            
            if saved_count > 0:
                logger.info(f"[ScraperSync] å³æ—¶ç¼“å­˜ {stock_code}: {saved_count} æ¡æ–°å¢")
                
        except Exception as e:
            # å³æ—¶ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•æ—¥å¿—
            logger.warning(f"[ScraperSync] å³æ—¶ç¼“å­˜å¤±è´¥ {stock_code}: {e}")


# å…¨å±€å•ä¾‹
_scraper_sync_service: Optional[ScraperSyncService] = None


def get_scraper_sync_service() -> ScraperSyncService:
    """è·å–åŒæ­¥æœåŠ¡å•ä¾‹"""
    global _scraper_sync_service
    if _scraper_sync_service is None:
        _scraper_sync_service = ScraperSyncService()
    return _scraper_sync_service
