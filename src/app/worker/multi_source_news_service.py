"""
å¤šæºæ–°é—»èšåˆæœåŠ¡
æ•´åˆå¤šä¸ªæ–°é—»æºï¼Œæä¾›ç»Ÿä¸€çš„æ–°é—»è·å–æ¥å£
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.database import get_mongo_db
from app.services.news_data_service import get_news_data_service
from app.worker.news_adapters.base import NewsSourceAdapter

logger = logging.getLogger(__name__)


class MultiSourceNewsService:
    """å¤šæºæ–°é—»èšåˆæœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        self.db = None
        self.news_service = None
        self.adapters: List[NewsSourceAdapter] = []
        self.batch_size = 10
        self.timeout = 30  # ç§’
        
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        logger.info("ğŸ”„ åˆå§‹åŒ–å¤šæºæ–°é—»èšåˆæœåŠ¡...")
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db = get_mongo_db()
        self.news_service = await get_news_data_service()
        
        # åˆå§‹åŒ–æ‰€æœ‰é€‚é…å™¨
        await self._initialize_adapters()
        
        logger.info(f"âœ… å¤šæºæ–°é—»èšåˆæœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.adapters)} ä¸ªæ–°é—»æº")
    
    async def _initialize_adapters(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ–°é—»æºé€‚é…å™¨"""
        import os
        
        # 1. AKShareé€‚é…å™¨ï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
        try:
            from app.worker.news_adapters.akshare_adapter import AKShareAdapter
            akshare = AKShareAdapter()
            await akshare.initialize()
            self.adapters.append(akshare)
            logger.info("âœ… AKShareé€‚é…å™¨å·²åŠ è½½")
        except Exception as e:
            logger.error(f"âŒ AKShareé€‚é…å™¨åŠ è½½å¤±è´¥: {e}")
        
        # 2. Alpha Vantageé€‚é…å™¨ï¼ˆéœ€è¦API Keyï¼‰
        if os.getenv("NEWS_SOURCE_ALPHA_VANTAGE", "true").lower() == "true":
            try:
                from app.worker.news_adapters.alpha_vantage_adapter import AlphaVantageAdapter
                alpha_vantage = AlphaVantageAdapter()
                if alpha_vantage.api_key:
                    self.adapters.append(alpha_vantage)
                    logger.info("âœ… Alpha Vantageé€‚é…å™¨å·²åŠ è½½")
                else:
                    logger.warning("âš ï¸ Alpha Vantage API Keyæœªé…ç½®ï¼Œè·³è¿‡")
            except Exception as e:
                logger.error(f"âŒ Alpha Vantageé€‚é…å™¨åŠ è½½å¤±è´¥: {e}")
        
        # 3. FinnHubé€‚é…å™¨ï¼ˆéœ€è¦API Keyï¼‰
        if os.getenv("NEWS_SOURCE_FINNHUB", "true").lower() == "true":
            try:
                from app.worker.news_adapters.finnhub_adapter import FinnHubAdapter
                finnhub = FinnHubAdapter()
                if finnhub.api_key:
                    self.adapters.append(finnhub)
                    logger.info("âœ… FinnHubé€‚é…å™¨å·²åŠ è½½")
                else:
                    logger.warning("âš ï¸ FinnHub API Keyæœªé…ç½®ï¼Œè·³è¿‡")
            except Exception as e:
                logger.error(f"âŒ FinnHubé€‚é…å™¨åŠ è½½å¤±è´¥: {e}")
        
        # 4. RSSé€‚é…å™¨
        if os.getenv("NEWS_SOURCE_RSS", "true").lower() == "true":
            try:
                from app.worker.news_adapters.rss_adapter import RSSAdapter
                rss = RSSAdapter()
                self.adapters.append(rss)
                logger.info("âœ… RSSé€‚é…å™¨å·²åŠ è½½")
            except Exception as e:
                logger.error(f"âŒ RSSé€‚é…å™¨åŠ è½½å¤±è´¥: {e}")
        
        logger.info(f"ğŸ“Š å…±åŠ è½½ {len(self.adapters)} ä¸ªæ–°é—»æºé€‚é…å™¨")
    
    async def sync_news_data(
        self,
        symbols: List[str] = None,
        max_news_per_stock: int = 20,
        force_update: bool = False,
        favorites_only: bool = True
    ) -> Dict[str, Any]:
        """
        åŒæ­¥æ–°é—»æ•°æ®ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            max_news_per_stock: æ¯åªè‚¡ç¥¨æœ€å¤§æ–°é—»æ•°é‡
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°
            favorites_only: æ˜¯å¦åªåŒæ­¥è‡ªé€‰è‚¡
            
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹å¤šæºæ–°é—»æ•°æ®åŒæ­¥...")
        
        stats = {
            "total_processed": 0,
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "source_stats": {},
            "start_time": datetime.utcnow(),
            "favorites_only": favorites_only,
            "errors": []
        }
        
        try:
            # 1. è·å–è‚¡ç¥¨åˆ—è¡¨
            if symbols is None:
                if favorites_only:
                    symbols = await self._get_favorite_stocks()
                    logger.info(f"ğŸ“Œ åªåŒæ­¥è‡ªé€‰è‚¡ï¼Œå…± {len(symbols)} åª")
                else:
                    stock_list = await self.db.stock_basic_info.find(
                        {}, {"code": 1, "_id": 0}
                    ).to_list(None)
                    symbols = [s["code"] for s in stock_list if s.get("code")]
                    logger.info(f"ğŸ“Š åŒæ­¥æ‰€æœ‰è‚¡ç¥¨ï¼Œå…± {len(symbols)} åª")
            
            if not symbols:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦åŒæ­¥æ–°é—»çš„è‚¡ç¥¨")
                return stats
            
            stats["total_processed"] = len(symbols)
            
            # 2. æ‰¹é‡å¤„ç†
            for i in range(0, len(symbols), self.batch_size):
                batch = symbols[i:i + self.batch_size]
                batch_stats = await self._process_batch(batch, max_news_per_stock)
                
                # æ›´æ–°ç»Ÿè®¡
                stats["success_count"] += batch_stats["success_count"]
                stats["error_count"] += batch_stats["error_count"]
                stats["news_count"] += batch_stats["news_count"]
                stats["errors"].extend(batch_stats["errors"])
                
                # åˆå¹¶æºç»Ÿè®¡
                for source, count in batch_stats.get("source_stats", {}).items():
                    stats["source_stats"][source] = stats["source_stats"].get(source, 0) + count
                
                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(symbols))
                logger.info(f"ğŸ“ˆ è¿›åº¦: {progress}/{len(symbols)} "
                           f"(æˆåŠŸ: {stats['success_count']}, æ–°é—»: {stats['news_count']})")
                
                # APIé™æµ
                if i + self.batch_size < len(symbols):
                    await asyncio.sleep(0.5)
            
            # 3. å®Œæˆç»Ÿè®¡
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
            logger.info(f"âœ… å¤šæºæ–°é—»æ•°æ®åŒæ­¥å®Œæˆ: "
                       f"æ€»è®¡ {stats['total_processed']} åªè‚¡ç¥¨, "
                       f"æˆåŠŸ {stats['success_count']} åª, "
                       f"è·å– {stats['news_count']} æ¡æ–°é—», "
                       f"è€—æ—¶ {stats['duration']:.2f} ç§’")
            
            # è¾“å‡ºå„æºç»Ÿè®¡
            for source, count in stats["source_stats"].items():
                logger.info(f"  ğŸ“° {source}: {count} æ¡æ–°é—»")
            
            # 4. ä¿å­˜åŒæ­¥å†å²è®°å½•
            try:
                await self._save_sync_history(stats)
            except Exception as e:
                logger.error(f"ä¿å­˜åŒæ­¥å†å²å¤±è´¥: {e}")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ å¤šæºæ–°é—»æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            stats["errors"].append({"error": str(e), "context": "sync_news_data"})
            return stats
    
    async def _process_batch(
        self,
        batch: List[str],
        max_news_per_stock: int
    ) -> Dict[str, Any]:
        """
        å¤„ç†ä¸€æ‰¹è‚¡ç¥¨çš„æ–°é—»è·å–
        
        Args:
            batch: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            max_news_per_stock: æ¯åªè‚¡ç¥¨æœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
        """
        batch_stats = {
            "success_count": 0,
            "error_count": 0,
            "news_count": 0,
            "source_stats": {},
            "errors": []
        }
        
        for symbol in batch:
            try:
                # ä»æ‰€æœ‰æºè·å–æ–°é—»
                all_news = await self._fetch_from_all_sources(symbol, max_news_per_stock)
                
                if all_news:
                    # å»é‡
                    unique_news = self._deduplicate_news(all_news)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    saved_count = await self._save_news(unique_news)
                    
                    batch_stats["success_count"] += 1
                    batch_stats["news_count"] += saved_count
                    
                    # ç»Ÿè®¡å„æºæ–°é—»æ•°é‡
                    for news in unique_news:
                        source = news.get("source", "unknown")
                        batch_stats["source_stats"][source] = batch_stats["source_stats"].get(source, 0) + 1
                    
                    logger.debug(f"âœ… {symbol} æ–°é—»åŒæ­¥æˆåŠŸ: {saved_count}æ¡")
                else:
                    logger.debug(f"âš ï¸ {symbol} æœªè·å–åˆ°æ–°é—»")
                    batch_stats["success_count"] += 1
                
                await asyncio.sleep(0.2)
                
            except Exception as e:
                batch_stats["error_count"] += 1
                error_msg = f"{symbol}: {str(e)}"
                batch_stats["errors"].append(error_msg)
                logger.error(f"âŒ {symbol} æ–°é—»åŒæ­¥å¤±è´¥: {e}")
                await asyncio.sleep(1.0)
        
        return batch_stats
    
    async def _fetch_from_all_sources(
        self,
        symbol: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        å¹¶è¡Œä»æ‰€æœ‰æºè·å–æ–°é—»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            limit: æ¯ä¸ªæºçš„æœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            æ‰€æœ‰æºçš„æ–°é—»åˆ—è¡¨
        """
        all_news = []
        
        # è·å–å¯ç”¨çš„é€‚é…å™¨
        available_adapters = [a for a in self.adapters if a.is_available()]
        
        if not available_adapters:
            logger.warning(f"âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ–°é—»æº")
            return all_news
        
        # å¹¶è¡Œè·å–
        tasks = [
            self._fetch_from_source(adapter, symbol, limit)
            for adapter in available_adapters
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆå¹¶ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, list):
                all_news.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"æº {available_adapters[i].source_name} è·å–å¤±è´¥: {result}")
        
        return all_news
    
    async def _fetch_from_source(
        self,
        adapter: NewsSourceAdapter,
        symbol: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        ä»å•ä¸ªæºè·å–æ–°é—»
        
        Args:
            adapter: æ–°é—»æºé€‚é…å™¨
            symbol: è‚¡ç¥¨ä»£ç 
            limit: æœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        try:
            news_list = await asyncio.wait_for(
                adapter.get_news(symbol, limit),
                timeout=self.timeout
            )
            adapter.record_success()
            return news_list
        except asyncio.TimeoutError:
            logger.warning(f"[{adapter.source_name}] è·å–è¶…æ—¶: {symbol}")
            adapter.record_error()
            return []
        except Exception as e:
            logger.error(f"[{adapter.source_name}] è·å–å¤±è´¥: {symbol} - {e}")
            adapter.record_error()
            return []
    
    def _deduplicate_news(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å»é‡æ–°é—»ï¼ˆç®€å•ç‰ˆæœ¬ï¼šæŒ‰æ ‡é¢˜ï¼‰
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–°é—»åˆ—è¡¨
        """
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title = news.get("title", "")
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        return unique_news
    
    async def _save_news(self, news_list: List[Dict[str, Any]]) -> int:
        """
        ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            ä¿å­˜çš„æ–°é—»æ•°é‡
        """
        if not news_list:
            return 0
        
        try:
            saved_count = await self.news_service.save_news_data(
                news_data=news_list,
                data_source="multi_source",
                market="CN"
            )
            return saved_count
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°é—»å¤±è´¥: {e}")
            return 0
    
    async def _get_favorite_stocks(self) -> List[str]:
        """
        è·å–æ‰€æœ‰ç”¨æˆ·çš„è‡ªé€‰è‚¡åˆ—è¡¨
        
        Returns:
            è‡ªé€‰è‚¡ä»£ç åˆ—è¡¨
        """
        try:
            favorites_docs = await self.db.user_favorites.find(
                {},
                {"favorites": 1, "_id": 0}
            ).to_list(None)
            
            stock_codes = set()
            for doc in favorites_docs:
                favorites = doc.get("favorites", [])
                for fav in favorites:
                    code = fav.get("stock_code")
                    if code:
                        stock_codes.add(code)
            
            return list(stock_codes)
            
        except Exception as e:
            logger.error(f"è·å–è‡ªé€‰è‚¡å¤±è´¥: {e}")
            return []
    
    async def _save_sync_history(self, stats: Dict[str, Any]):
        """
        ä¿å­˜åŒæ­¥å†å²è®°å½•
        
        Args:
            stats: åŒæ­¥ç»Ÿè®¡æ•°æ®
        """
        try:
            import uuid
            
            history_record = {
                "sync_id": str(uuid.uuid4()),
                "sync_time": stats["start_time"],
                "sync_type": "multi_source",
                "status": "success" if stats["error_count"] == 0 else "partial_success",
                "total_stocks": stats["total_processed"],
                "success_count": stats["success_count"],
                "error_count": stats["error_count"],
                "news_count": stats["news_count"],
                "source_stats": stats["source_stats"],
                "duration": stats["duration"],
                "errors": stats["errors"][:10] if stats["errors"] else []  # åªä¿å­˜å‰10ä¸ªé”™è¯¯
            }
            
            await self.db.news_sync_history.insert_one(history_record)
            logger.debug(f"åŒæ­¥å†å²è®°å½•å·²ä¿å­˜: {history_record['sync_id']}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŒæ­¥å†å²å¤±è´¥: {e}")


# å…¨å±€æœåŠ¡å®ä¾‹
_multi_source_news_service = None


async def get_multi_source_news_service() -> MultiSourceNewsService:
    """è·å–å¤šæºæ–°é—»æœåŠ¡å®ä¾‹"""
    global _multi_source_news_service
    
    if _multi_source_news_service is None:
        _multi_source_news_service = MultiSourceNewsService()
        await _multi_source_news_service.initialize()
    
    return _multi_source_news_service
