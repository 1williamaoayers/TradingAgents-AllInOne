"""
Playwrightçˆ¬è™«APIé€‚é…å™¨
è°ƒç”¨ playwriteOCR é¡¹ç›®çš„ RESTful API è·å–æ–°é—»
æ”¯æŒå³æ—¶åŒå†™ï¼šè·å–æ–°é—»æ—¶è‡ªåŠ¨å¼‚æ­¥ç¼“å­˜åˆ°æ•°æ®åº“
"""
import asyncio
import logging
import os
import aiohttp
from typing import List, Dict, Any
from datetime import datetime

from app.worker.news_adapters.base import NewsSourceAdapter, create_standard_news_item

logger = logging.getLogger(__name__)

# å³æ—¶åŒå†™å¼€å…³ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
ENABLE_INSTANT_CACHE = os.getenv("SCRAPER_INSTANT_CACHE", "true").lower() == "true"


class ScraperAdapter(NewsSourceAdapter):
    """Playwrightçˆ¬è™«APIé€‚é…å™¨ï¼ˆæ”¯æŒå³æ—¶åŒå†™ï¼‰"""
    
    def __init__(self, api_url: str = None):
        super().__init__("scraper")
        # Dockerå®¹å™¨å†…ä½¿ç”¨ç½‘å…³IPè®¿é—®å®¿ä¸»æœº
        # å¯é€šè¿‡ç¯å¢ƒå˜é‡SCRAPER_API_URLè¦†ç›–
        default_url = os.getenv("SCRAPER_API_URL", "http://172.20.0.1:9527")
        self.api_url = api_url or default_url

    
    async def get_news(self, symbol: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨çš„æ–°é—»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç æˆ–å…¬å¸åç§°
            limit: æœ€å¤§æ–°é—»æ•°é‡
            
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        try:
            # è°ƒç”¨çˆ¬è™«API
            url = f"{self.api_url}/api/v1/news"
            params = {
                "keyword": symbol,
                "limit": limit
                # sources ä¸ä¼ ï¼Œé»˜è®¤é‡‡é›†å…¨éƒ¨ 8 ä¸ªæº
            }
            
            async with aiohttp.ClientSession() as session:
                # çˆ¬è™«ä»»åŠ¡è€—æ—¶è¾ƒé•¿ï¼Œè®¾ç½®ä¸º 900ç§’ (15åˆ†é’Ÿ) è¶…æ—¶
                async with session.get(url, params=params, timeout=900) as resp:
                    if resp.status != 200:
                        logger.error(f"[Scraper] APIè¿”å›é”™è¯¯: {resp.status}")
                        return []
                    
                    data = await resp.json()
                    
                    if not data.get("success"):
                        logger.error(f"[Scraper] APIå¤±è´¥: {data.get('error')}")
                        return []
                    
                    raw_news_list = data.get("data", [])
                    
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    news_list = []
                    for raw_news in raw_news_list:
                        normalized = self.normalize_news(raw_news)
                        news_list.append(normalized)
                    
                    logger.info(f"[Scraper] {symbol} é‡‡é›†åˆ° {len(news_list)} æ¡æ–°é—»")
                    
                    # ğŸ”¥ å³æ—¶åŒå†™ï¼šå¼‚æ­¥ç¼“å­˜åˆ°æ•°æ®åº“ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
                    if ENABLE_INSTANT_CACHE and news_list:
                        asyncio.create_task(self._cache_to_db(news_list, symbol))
                    
                    return news_list
                    
        except aiohttp.ClientError as e:
            logger.error(f"[Scraper] ç½‘ç»œé”™è¯¯: {e}")
            self.record_error()
            return []
        except Exception as e:
            logger.error(f"[Scraper] è·å–æ–°é—»å¤±è´¥: {symbol} - {e}")
            self.record_error()
            return []
    
    async def _cache_to_db(self, news_list: List[Dict[str, Any]], symbol: str):
        """
        å³æ—¶åŒå†™ï¼šå¼‚æ­¥ç¼“å­˜æ–°é—»åˆ°æ•°æ®åº“ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            symbol: è‚¡ç¥¨ä»£ç 
        """
        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
            from app.worker.scraper_sync_service import get_scraper_sync_service
            
            service = get_scraper_sync_service()
            await service.cache_news_immediately(news_list, symbol)
            
        except Exception as e:
            # å³æ—¶ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œåªè®°å½•è­¦å‘Šæ—¥å¿—
            logger.warning(f"[Scraper] å³æ—¶ç¼“å­˜å¤±è´¥ï¼ˆä¸å½±å“åˆ†æï¼‰: {symbol} - {e}")
    
    def normalize_news(self, raw_news: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–çˆ¬è™«APIè¿”å›çš„æ–°é—»ä¸ºç»Ÿä¸€æ ¼å¼
        
        Args:
            raw_news: APIåŸå§‹æ–°é—»ï¼ˆå·²æ˜¯æ ‡å‡†æ ¼å¼ï¼‰
            
        Returns:
            æ ‡å‡†åŒ–çš„æ–°é—»å­—å…¸
        """
        # APIå·²ç»è¿”å›æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
        return create_standard_news_item(
            symbol=raw_news.get("symbol", ""),
            title=raw_news.get("title", ""),
            source=raw_news.get("source", "Scraper"),
            summary=raw_news.get("summary", ""),
            content=raw_news.get("content", ""),
            source_type="scraper",
            url=raw_news.get("url", ""),
            publish_time=raw_news.get("publish_time") or datetime.utcnow(),
            sentiment=raw_news.get("sentiment", "neutral"),
            relevance_score=raw_news.get("relevance_score", 0.8),
            tags=raw_news.get("tags", [])
        )

